Large Language Models for Networking:
Applications, Enabling Techniques, and Challenges
Yudong Huang, Hongyang Du, Xinyuan Zhang, Dusit Niyato, Fellow, IEEE,
Jiawen Kang, Zehui Xiong, Shuo Wang, and Tao Huang, Senior Member, IEEE
Abstract‚ÄîThe rapid evolution of network technologies and
the growing complexity of network tasks necessitate a paradigm
shift in how networks are designed, conÔ¨Ågured, and managed.
With a wealth of knowledge and expertise, large language models
(LLMs) are one of the most promising candidates. This paper
aims to pave the way for constructing domain-adapted LLMs for
networking. Firstly, we present potential LLM applications for
vertical network Ô¨Åelds and showcase the mapping from natural
language to network language. Then, several enabling technolo-
gies are investigated, including parameter-efÔ¨Åcient Ô¨Ånetuning and
prompt engineering. The insight is that language understanding
and tool usage are both required for network LLMs. Driven
by the idea of embodied intelligence, we propose the ChatNet, a
domain-adapted network LLM framework with access to various
external network tools. ChatNet can reduce the time required
for burdensome network planning tasks signiÔ¨Åcantly, leading to
a substantial improvement in processing efÔ¨Åciency. Finally, key
challenges and future research directions are highlighted.
Index Terms‚ÄîLarge Language Models, Generative AI, Intent-
driven Networking, Network Intelligence.
I. INTRODUCTION
Generative artiÔ¨Åcial intelligence (AI) technology is regarded
as one of the most inspiring breakthroughs in the intelligent
era. Through outstanding reasoning, generalization, and emer-
gent abilities, large language models (LLMs) with billions of
model parameters have shown great commercial value and
technical potential, such as text-to-text, text-to-image, and text-
to-code. The ChatGPT gains 1 million users within just one
week, and open source LLMs (e.g., GPT-2, LLaMA, and
BLOOM) are emerging one after another.
In particular, domain-adapted LLMs have been successfully
Y. Huang and X. Zhang are with the State Key Laboratory of Networking
and Switching Technology, BUPT, Beijing, 100876, P.R. China (e-mail:
hyduni@bupt.edu.cn, zhangxinyuan0181@bupt.edu.cn).
H. Du and D. Niyato are with the School of Computer Science
and Engineering, Nanyang Technological University, Singapore (e-mail:
hongyang001@e.ntu.edu.sg, dniyato@ntu.edu.sg).
J. Kang is with the School of Automation, Guangdong University of
Technology, China (e-mail: kavinkang@gdut.edu.cn).
Z. Xiong is with Information Systems Technology and Design (ISTD)
Pillar, Singapore University of Technology and Design, Singapore (email:
zehui xiong@sutd.edu.sg).
S. Wang and T. Huang are with the State Key Laboratory of Network-
ing and Switching Technology, BUPT, Beijing, 100876, P.R. China, and
the Purple Mountain Laboratories, Nanjing, 211111, P.R. China (e-mail:
shuowang@bupt.edu.cn, htao@bupt.edu.cn).
utilized in robot embodied intelligence12, chip design3, and
protein structure generation45. Since LLMs are initially trained
on vast amounts of internet data, domain-adapted refers to the
process of adapting a general-purpose language model with
domain-speciÔ¨Åc datasets to enhance the model‚Äôs performance
in a particular Ô¨Åeld. Generative LLMs can compress infor-
mation features and vectorize massive knowledge as tokens,
thereby aiding or even replacing humans in conceptual under-
standing, logical reasoning, and decision-making. Intuitively,
this makes it possible to efÔ¨Åciently complete network tasks
through natural language interaction with intelligent machines,
while implementing domain-adapted LLMs for vertical net-
working Ô¨Åelds becomes an important research challenge.
Before the birth of LLMs, many research efforts trained
task-speciÔ¨Åc AI models to express the paradigm of intent-
driven networking (IDN). For instance, by leveraging a
sequence-to-sequence learning model, a chatbot named Lumi6
was proposed to extract entities from the operator utterances,
where these entities are further translated into network intent
language and deployable network policies. To reduce the con-
Ô¨Åguration complexity of Access Control List (ACL) rules, Lan-
guage for ACL Intents (LAI) [1] was designed with speciÔ¨Åc
grammar that contains three parts of region, requirement, and
command. In [2], the authors realized automated management
of heterogeneous vendor-speciÔ¨Åc devices. It adopts the Bidi-
rectional Encoder Representations from Transformers (BERT)
model and learns directly from various devices‚Äô manuals to
produce uniÔ¨Åed network data models.
Although these schemes perform well in certain network
tasks and scenarios, there are several limitations: 1) Lack of
generalization. An AI model trained on a speciÔ¨Åc dataset may
perform poorly on new or unseen network tasks. The lack
of generalization ability prevents the AI model from being
deployed in real network scenarios. 2) Huge training costs.
Training takes days or even months, as well as huge computing
resources and labor costs, making it uneconomical to build
an AI model from scratch. 3) Hard to integrate. Existing
intent-driven methods are limited to semantic conversion,
1Google PaLM-E: https://palm-e.github.io/
2ROS-LLM: https://github.com/Auromix/ROS-LLM
3NVIDIA
ChipNeMo:
https://research.nvidia.com/publication/2023-
10 chipnemo-domain-adapted-llms-chip-design
4AlphaFold: https://github.com/google-deepmind/alphafold
5ESM-2: https://github.com/facebookresearch/esm
6Lumi Chatbot: https://lumichatbot.github.io/#/
This article has been accepted for publication in IEEE Network. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/MNET.2024.3435752
¬© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Tsinghua University. Downloaded on September 02,2024 at 10:50:17 UTC from IEEE Xplore.  Restrictions apply. 

‚Ä¶
Network design
Network diagnosis
Network Configuration
Network Security
(a) LLM applications for vertical network fields
(b) Generative Pretrained Transformer
(c) Domain-adapted
LLMs for networking
Pretrained
LLMs
Trillions tokens 
of Internet data
105 ‚Äì 106 GPU hrs
Finetuning with massive 
network knowledge
Domain-
adapted LLMs
Thousands GPU hrs
ChatNet
Inference
Mechanism
Plug-and-play
tool kits
‚Ä¶
Prompt engineering
Support
¬∑ Zero shot
¬∑ Few shot
¬∑ CoT, RAG
LLaMA
‚Ä¶
Prompts
Results
Prompts
Results
Prompts
Results
Prompts
Results
Give me a network capacity
planning scheme, traffic
matrix is ‚Ä¶ constraints
are‚Ä¶ minimize the costs ‚Ä¶
Sure, I'd be happy to help!
The following is the scheme
15G
10G
20G
5G
10G
5G
20G
15G
Application x has a lot of 
packet loss, please check 
the network status and 
output a diagnostic report‚Ä¶
The link L1 on switch S2 is 
interrupted. May I migrate x
to the backup path?
x
Backup path
S2
L1
There is a new Cisco device 
with brand serial number‚Ä¶
Please configure it to access 
the SDN controller
Manual Commands
Config
According to the device
manual‚Ä¶ the configuration
commands are </>‚Ä¶ done!
Capture traffic of mirror port P, 
use Wireshark to parse packet 
protocol, issue firewall policies 
to prevent malicious attacks
Parser/Tools
Policy
Traffic
Deny
Ok! First, the captured traffic
is ‚Ä¶ After parse ‚Ä¶ Finally,
the policies are generated ‚Ä¶
I think the network status
is
good
‚ü®ùê∏ùëÇ‚ü©
ùëÜ
is
good
Layer 1
Layer L
Iter 1
Iter 2
Iter 3
‚Ä¶ ‚Ä¶
‚Ä¶ ‚Ä¶
‚Ä¶
‚Ä¶
‚®Å
X"
Q = X" ) ùëä#
"
K = X" ) ùëä$
"
V = X" ) ùëä%
"
ùê¥ttention ùëÑ, ùêæ, ùëâ=
softmax #$!
&! V
Input
LayerNorm
Output
‚®Å
MLP
LayerNorm
QKV Linear
Multi-Head Attention
API
weight matrix
FFN
Embeddings
Frozen
weights
Tunable
Fig. 1: Applications, mechanisms, and enabling techniques for domain-adapted network LLMs. (a) Potential network LLM
applications. (b) Working mechanisms of Generative Pretrained Transformer. (c) Finetuning process and prompt engineering.
which struggles to integrate with a wide range of off-the-shelf
techniques (e.g., network simulator and search engine) and
tools (e.g., solver, code interpreter, and visualization platform).
The key point is that network language exists formal rules,
protocols, mathematical expressions, and formula constraints,
rather than plain text of natural language. Fortunately, LLMs
are expected to continuously learn updated world knowledge
and comprehensively utilize tools through application pro-
gramming interface (API). Thus, arbitrary complex network
tasks could be completed by calling LLMs with a combination
of plug-and-play functional components. In this paper, we
target to pave the way for constructing domain-adapted LLMs
for networking, including applications in network design, net-
work diagnosis, network conÔ¨Åguration, and network security.
Typically, IDN [3] emphasizes managing and conÔ¨Åguring
networks through programmable northbound interfaces within
the SDN/NFV framework, where intents are closely related to
the terms of policy and conÔ¨Åguration. This paper broadens the
scope of intent to ubiquitous network tasks, especially Ô¨Ålling
the gaps in network design, such as network knowledge Q&A,
protocol design, code generation, trafÔ¨Åc generation, and net-
work simulation. Here, the outcome of intent takes on a more
diverse form, including text, images, protocols, codes, datasets,
and schemes. We envision that domain-adapted network LLMs
with proÔ¨Åciency in external tools ( i.e., network embodied
intelligence) will be ubiquitous and reshape future network
infrastructure. The main contributions of this article are:
‚Ä¢ We analyze the features of natural language and network
language, and showcase typical intent conversion patterns.
‚Ä¢ We present the enabling techniques of domain-adapted
LLMs for networking, including pre-training, Ô¨Ånetuning, in-
ference, and prompt engineering.
‚Ä¢ We propose a conceptual framework, named ChatNet,
with essential components of analyzer, planner, calculator, and
executor, to express the LLM-based network intelligence.
‚Ä¢ We conduct a case study of LLM-based network planning,
where ChatNet can understand intents and generate visual ca-
pacity schemes with changing trafÔ¨Åc matrices and constraints.
The rest of the article is organized as follows. We commence
with the applications of LLMs for networking, and exhibit the
instances of network language. Next, we analyze the enabling
techniques of domain-adapted LLMs for networking. Then the
key functional components of ChatNet are detailed. Following
that, we give the case study and analyze the challenges.
Finally, we draw the main conclusions.
II. APPLICATIONS OF LLM IN NETWORKING
This section overviews the promising LLM applications for
vertical network Ô¨Åelds, emphasizing the discrepancy between
natural language and network language.
A. Potential LLM Applications for Vertical Network Fields
Developing network systems and managing network infras-
tructure are knowledge-intensive and labor-intensive indus-
tries, which necessitate a lot of expert experience and manual
operations. Previously, network intelligence was fragmented,
residing in disparate small models, such as Deep Neural
Network, Long Short-Term Memory, and Deep Reinforcement
This article has been accepted for publication in IEEE Network. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/MNET.2024.3435752
¬© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Tsinghua University. Downloaded on September 02,2024 at 10:50:17 UTC from IEEE Xplore.  Restrictions apply. 

TABLE I: Mapping of natural language intents to typical network language implementations
Natural Language (Intent)
Network Language
Characteristics
Examples of Mapping Relationships
‚ÄúRestrict access to the server at
192.168.1.5 from all external IPs‚Äù
Access Control List
(ACL)
Control trafÔ¨Åc based on
IPs, protocols, ports, etc.
deny ip any 192.168.1.5.
‚ÄúSet up the new router to prioritize VoIP
trafÔ¨Åc for better call quality‚Äù
Command Line Interface
(CLI) / Policy
ConÔ¨Ågure network
devices
class-map VOIP,
policy-map VOIP-Policy
‚ÄúAutomatically adapt to changes in
topology without manual reconÔ¨Åguration‚Äù
YANG Model / XML /
JSON
DeÔ¨Åne data structure for
network management.
‚ü®interface‚ü©
‚ü®name‚ü©10GE 1/0/1‚ü®/name‚ü©
‚ÄúParse all TCP packets and detect
malicious and spoofed connections‚Äù
Protocols (e.g., TCP,
UDP, IP)
DeÔ¨Åne secure data
exchange rules
IP header| TCP header| Payload
‚ÄúEnsure the network does not exceed 80%
capacity during 9 AM to 5 PM‚Äù
Mathematical Formulas
and Constraints
Constraints manage
network performance
if (time‚â•9 AM and time ‚â§5 PM)
max_load ‚â§0.8 * total_capacity
Learning. Each model was independently deployed within spe-
ciÔ¨Åc environments, such as intelligent assistants for customer
service, adaptive routing algorithms for improving quality of
service (QoS), and deÔ¨Ånite conÔ¨Åguration synthesis modules for
alleviating manual errors. Currently, LLMs possess extensive
world knowledge and can engage in multi-turn dialogues with
humans via a natural language interface. The capability to un-
derstand human intentions and self-modify based on prompts
is a feature that previous models lacked. LLMs promise to
unify network intelligence through common natural language
interfaces, making the network itself a generalist to understand
the knowledge and master the tools. As shown in Fig. 1(a),
we classify potential applications of LLMs in network vertical
Ô¨Åelds as follows.
1) Network Design: By processing vast datasets encom-
passing network performance metrics, equipment speciÔ¨Åca-
tions, and historical design patterns, LLMs can assist en-
gineers in equipment selection, network planning7, network
data analysis8, code generation9, trafÔ¨Åc generation10, pro-
tocol formulation11, reproducing network research results12,
and many other aspects of network design. In equipment
selection, LLMs could analyze compatibility requirements,
performance benchmarks, and cost considerations, providing
recommendations that align with speciÔ¨Åc network objectives.
For network planning, LLMs may simulate various network
schemes, predict potential bottlenecks, and suggest optimal
layouts that balance efÔ¨Åciency, scalability, and resilience.
2) Network Diagnosis: Troubleshooting is a tedious and
burdensome task for network operators. Especially in large-
scale wide-area networks, it requires coordination between dif-
ferent departments across multiple regions, while applications
still suffer from inexplicable network failures or performance
degradation, and are threatened with hundreds of millions of
7LossLeaP
for
trafÔ¨Åc
prediction
and
capacity
forecasting:
https://github.com/alcoimdea/LossLeaP
8M. Kotaru. ‚ÄùAdapting foundation models for operator data analytics,‚Äù in
HotNets, 2023.
9https://github.com/microsoft/NeMoEval
10X. Jiang, et al. ‚ÄùGenerative, high-Ô¨Ådelity network traces,‚Äù in HotNets,
2023.
11P. Sharma, et al. ‚ÄùPROSPER: Extracting protocol speciÔ¨Åcations using
large language models,‚Äù in HotNets, 2023.
12Q. Xiang, et al. ‚ÄùToward reproducing network research results using large
language models,‚Äù in HotNets, 2023.
Ô¨Ånancial losses. By integrating LLMs into network diagnostic
systems13, LLMs are capable of generating fault reports based
on network status information, accelerating fault location, and
giving reasonable processing suggestions based on the report
analysis and historical operational data.
3) Network ConÔ¨Åguration: There are a large number of
heterogeneous devices in the network, e.g., switches, routers,
and middleware. Due to vendor-speciÔ¨Åc device models, sig-
niÔ¨Åcant expert effort is required to learn the user manuals,
collect suitable commands, validate conÔ¨Åguration templates,
and map template parameters to the controller database. In
this process, even a single ACL misconÔ¨Åguration may lead to
network disruptions. Considering the growing heterogeneous
cloud networks with plenty of computing and storage devices
that also need to be managed, a uniÔ¨Åed natural language
conÔ¨Åguration interface14 is essential for simplifying the con-
Ô¨Åguration process and enabling self-conÔ¨Ågured networks.
4) Network Security: Networks often face various potential
security issues, such as distributed denial-of-service (DDoS)
attacks, address spooÔ¨Ång, and data leakage. Protecting the
network from malicious attacks combines a series of oper-
ations, such as security assessment, vulnerability scanning,
intrusion detection and defense. LLMs are powerful interactive
platforms to access diverse security tools and systems15. For
instance, guided by logically rigorous prompts, LLMs may
complete the abnormal trafÔ¨Åc denying tasks by calling the
parse tool of Wireshark and updating the policy to Ô¨Årewalls.
Moreover, a neural API retriever16 could be trained to select
appropriate security tools. To guarantee effectiveness, high-
quality instruction tuning datasets and ToolBench17 are essen-
tial to empower LLMs to master thousands of diverse tools.
B. From Natural Language to Network Language
As depicted in Table I, different from the plain text of
natural language, the network language contains more non-
13Juniper
Marvis:
https://www.juniper.net/us/en/products/cloud-
services/virtual-network-assistant.html
14Huawei NAssim: https://github.com/AmyWorkspace/nassim
15Google
Cloud
Security
AI
Workbench
and
Sec-PaLM
2:
https://cloud.google.com/security/ai
16Y. Qin, et al. ‚ÄúToolLLM: Facilitating Large Language Models to Master
16000+ Real-world APIs,‚Äù in ICLR, 2024, pp. 1-24.
17https://github.com/OpenBMB/ToolBench.
This article has been accepted for publication in IEEE Network. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/MNET.2024.3435752
¬© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Tsinghua University. Downloaded on September 02,2024 at 10:50:17 UTC from IEEE Xplore.  Restrictions apply. 

standardized formats and symbols, from the high-level man-
agement policy to the low-level Access Control List (ACL),
Command Line Interface (CLI), and data modeling language
(e.g., YANG model, XML, and JSON). Traditional mapping
methods are restricted to formalized translations, e.g., entity
abstraction and template Ô¨Ålling. In contrast, LLMs can pro-
vide better network QoE performance by offering customized
responses to speciÔ¨Åc human-related natural language inputs.
Moreover, the network language has domain-speciÔ¨Åc nouns,
protocols, and rules, as well as mathematical constraints,
where LLMs are prone to ‚Äúillusion‚Äù due to ambiguous con-
cepts or ‚Äúbabbling‚Äù due to forgetting the relevance of the
context. Thus, our Ô¨Årst insight is that we can Ô¨Ånetune the LLMs
with massive network knowledge to enable domain-adapted
network LLMs, and Retrieval Augment based on accessing
external documents (e.g., device manuals and status logs) will
beneÔ¨Åt mapping the natural language to the network language.
In addition, completing network tasks are complex and
error-prone process, which requires not only semantics cor-
rectness but also being practically deployable. To address this
issue, the intent reÔ¨Ånement [7] was proposed to guarantee
the accuracy and completeness of the translation from the
declarative intent to network primitives, with methods like
Bi-LSTM and knowledge-graph. The network veriÔ¨Åcation
[8] further checks the conÔ¨Çicting policies and validates the
feasibility of results with various network tools. However,
assembling disparate tools and approaches in intent-driven
networking is still challenging. In this paper, we consider the
generative pre-trained transformer (GPT) techniques of LLMs
to be powerful enough to learn the usage of tools, as GPT-
4 has released the assistant APIs18 for easily constructing
customized GPT applications. Thus, our second insight is
that domain-adapted network LLMs can access external tools
[9], such as search engines, data analyzers, mathematical
solvers, and network tools, to automate any complex tasks,
such as processing time series data, parsing protocols, and
constructing mathematical models. Then, we call these kinds
of network LLMs as ChatNet.
III. DOMAIN-ADAPTED LLMS FOR NETWORKING
Motivated by the above issues and trends, this section
analyzes the enabling techniques for network LLM, especially
speciÔ¨Åc design problems for LLMs to make the LLM models
optimized and suitable for network applications, and several
assessment methods in prompt engineering are derived.
A. Enabling Techniques for Network LLMs
We divide the enabling techniques for network LLMs into
three categories: pre-training, Ô¨Ånetuning, and inference. The
pre-training part presents the working mechanism of LLMs.
Based on open-source vanilla LLMs, Ô¨Ånetuning is the most
important step in establishing domain-adapted network LLMs.
1) Generative Pre-trained Transformer: The training of
LLMs involves large-scale unsupervised learning, where the
18OpenAI assistants API: https://platform.openai.com/docs/assistants/overview
model is pre-trained on extensive text corpora (‚àºPBs) for
105 ‚àº106 GPU hours, learning to predict the next word
in a sentence. As shown in Fig. 1(b), the input is word
embeddings, which goes through all layers of the GPT. Each
layer mainly consists of a multi-head attention module and
a position-wise feed-forward network (FFN). The multi-head
attention mechanism utilizes multiple attention processes to
simultaneously computes the relevance of each word in a
sentence to every other word, using a set of Query (Q), Key
(K), and Value (V ) vectors and the intermediate result xl.
The outputs of these attention heads are then concatenated
and linearly transformed to produce next-token probabilities.
A single training run would cost millions of dollars, while
each thousand API calls cost less than one dollar for users.
2) Parameter-efÔ¨Åcient Finetuning: Since training LLMs
from scratch is prohibitively costly and calling third-party
LLMs via APIs poses data security risks, Ô¨Ånetuning open-
source LLMs is a viable candidate for building domain-
adapted network LLMs. As shown in Fig. 1(c), parameter-
efÔ¨Åcient Ô¨Ånetuning refers to freezing most of the pretrained
weights to adjust speciÔ¨Åc layers or adding additional tunable
parameters. For instance, Low-Rank Adaptation (LoRA) intro-
duces low-rank matrices to approximate the changes needed in
the model‚Äôs weights. Compared to full-parameter approaches,
LoRA strikes a balance between maintaining the general
capabilities of the original model and adapting it to speciÔ¨Åc
network domains and network tasks. A Data-driven low-rank
networking adaptation (DDLRNA)19 method has been recently
proposed to Ô¨Ånetune LLMs for both prediction and decision-
making tasks, and the effectiveness is proven across three
networking-related use cases, including viewport prediction,
adaptive bitrate streaming, and cluster job scheduling.
Particularly, the success of Ô¨Ånetuning largely depends on the
quality of data source and instruction datasets. Network data
collection encompassed a variety of operational reports, user
manuals, scripting languages, protocol descriptions, debug-
ging logs, and conÔ¨Åguration Ô¨Åles. Standardizing the datasets
includes a series of processes, such as cleaning, Ô¨Åltering,
categorization, normalization, and anonymization. Moreover,
a domain-adapted tokenizer can be trained to improve the
tokenization efÔ¨Åciency, by adding new tokens for domain-
speciÔ¨Åc network terms, such as keywords commonly found
in network protocols. To perform supervised Ô¨Ånetuning on the
domain-adapted network LLMs, high-quality network instruc-
tion datasets need to be established for speciÔ¨Åc network tasks.
3) Context-aware Inference: More than a mapper from
natural language to network language, domain-adapted net-
work LLMs have unprecedented in-context learning and multi-
turn dialogue capabilities. Context-aware inference means that
we can continuously provide new prompts to guide LLMs to
perform logical reasoning, avoiding the repeated construction
of preconditions and environments. Recently, the GPT-4 Turbo
model has supported a context window of 128K tokens,
19D. Wu, et al. ‚ÄúNetLLM: Adapting Large Language Models for Network-
ing,‚Äù in ACM SIGCOMM, 2024.
This article has been accepted for publication in IEEE Network. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/MNET.2024.3435752
¬© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Tsinghua University. Downloaded on September 02,2024 at 10:50:17 UTC from IEEE Xplore.  Restrictions apply. 

TABLE II: An overview of studies for LLMs with intent-driven networking (IDN) and network embodied intelligence (NEI).
Article
Contributions
IDN
LLMs
Finetuning
Prompt
NEI
LLM for
wireless [4]
Introduce applications of LLMs in future wireless communications,
including designing, training, testing, and deploying Telecom LLMs.
‚àö
‚àö
AIGN [5]
A generative network system that can generate customized network
solutions with the diffusion model-based learning approach.
‚àö
‚àö
NetGPT [6]
A collaborative cloud-edge methodology towards personalized LLM
services and native-AI network architecture.
‚àö
‚àö
‚àö
Ours ChatNet
Introduce domain-adapted LLMs for vertical network Ô¨Åelds and
propose the network embodied intelligence.
‚àö
‚àö
‚àö
‚àö
‚àö
where more than 300 pages of text can be Ô¨Åtted in a single
prompt. This opens up possibilities for LLM-based network
applications with massive information analysis and processing.
The ‚ÄúEmergent‚Äù abilities of LLMs have great potential to
intelligently create novel designs, mechanisms, and protocols
even unseen in existing network environments.
B. Prompt Engineering for Assessment
Effective prompt engineering is crucial for maximizing the
potential of network LLMs in various applications, involving
crafting questions or statements that guide the LLMs to pro-
duce the desired output. Especially for complex network tasks,
the prompt should be clear with sufÔ¨Åcient context, such as
satisfying speciÔ¨Åc constraints and response formats. Multiple
choice questions are a widely accepted assessment format for
LLMs. The authors of [10] collected a TeleQnA dataset with
10000 questions and answers, serving as an evaluation tool for
assessing the knowledge of LLMs in the telecommunications
domain. In [11], a NetEval dataset with 5732 questions was
constructed to measure the comprehensive capabilities of 26
publicly available LLMs in network operations. The following
are several commonly used prompt methods.
1) Zero Shot and Few Shot: Zero-shot prompts only
contain the task description and test questions, which are suit-
able for simple networking tasks, such as explaining concepts
in technical speciÔ¨Åcations. Recent studies20 hint that LLMs
are zero-shot reasoners. The few-shot prompts are slightly
more complex. It includes a small set of examples (usually
one to three) that demonstrate the desired task or answer
format. These examples serve as a mini-training set, guiding
the model to understand the context and the speciÔ¨Åc nature of
the response required. In the network Ô¨Åelds, these examples
might be brief descriptions of network conÔ¨Ågurations, trou-
bleshooting scenarios, or protocol interactions, followed by
the query that requires a similar response.
2) Chain of Thought: Chain-of-Thought (CoT) [12] en-
ables LLMs to tackle complex arithmetic, commonsense, and
symbolic reasoning tasks. This approach encourages the LLMs
to follow a step-by-step reasoning process, akin to how a
human might break down a problem. By using CoT prompting,
LLMs can be guided to systematically analyze a networking
problem. For example, in diagnosing a network issue, the
20K., Takeshi, et al. ‚ÄùLarge language models are zero-shot reasoners,‚Äù in
NIPS, 2022.
prompt can be engineered to lead the model through a series
of diagnostic steps, considering various factors like network
topology, hardware status, and software conÔ¨Ågurations. More-
over, CoT prompts can be designed to facilitate the cascading
use of plug-and-play network tools, empowering LLMs from
mere answer generators into network experts who are capable
of tools using, logical reasoning, and problem-solving.
3)
Retrieval-Augmented
Generation:
Retrieval-
Augmented
Generation
(RAG)21
combines
LLMs
with
information retrieval techniques, notably vector retrieval, to
enhance the model‚Äôs memory and factual accuracy. When
a query is received, the model Ô¨Årst performs a semantic
search across a vast database, selecting text pieces that are
semantically relevant to the query. This selection is facilitated
by
semantic
indexing,
which
efÔ¨Åciently
organizes
and
retrieves data based on its meaning rather than just keyword
matching. The retrieved information is then fed into the LLM
as a part of the prompt, effectively providing the model
with a context-rich background to generate more informed
and accurate responses. With the RAG, we can introduce
up-to-date datasets (e.g., iterative standard drafts and updated
maintenance logs) to LLMs dynamically, eliminating the need
for constant retraining of LLMs with new samples.
IV. CHATNET FRAMEWORK
After domain-adapted enhancement, it may still be a step
away from real implementation of the ChatNet, as knowledge
understanding alone does not directly derive the ability to
use tools. This section provides an in-depth analysis of the
key components required to improve proÔ¨Åciency in utilizing
network tools and proposes the ChatNet Framework. Addition-
ally, we compare ChatNet with other recent studies in Table
II, followed by a case study in network planning scenarios.
A.
Essential Components of ChatNet
The utilization of tools is a key indicator of advanced
intelligence, as demonstrated in the behaviors of the human
and the robot embodied intelligence [13]. In the same vein,
the ChatNet should master both the language understanding
and the tool usage, based on the following four fundamental
modules of analyzer, planner, calculator, and executor.
21Y. Gao, et al. ‚ÄùRetrieval-augmented generation for large language models:
A Survey,‚Äù in arXiv 2312.10997, 2023.
This article has been accepted for publication in IEEE Network. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/MNET.2024.3435752
¬© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Tsinghua University. Downloaded on September 02,2024 at 10:50:17 UTC from IEEE Xplore.  Restrictions apply. 

User: Tasks, Status, Constraints
-You are a network capacity planner‚Ä¶
-The traffic matrices are as attached‚Ä¶
-The constraints are bandwidth capacity 
constraints and optical capacity constraints‚Ä¶
-The target is to minimize the costs ‚Ä¶ ‚Ä¶
-Draw the IP and optical network topology.
Upload Files
ChatNet
‚Ä¶ ‚Ä¶
L = { ùêø!, ùêø", ùêø# }
Step 1: Read the first traffic matrix in the file.
Step 2: Calculate the network capacity scheme.
Step 3: Draw the IP and optical network topology.
Step 4: Read the second traffic matrix in the file.
‚Ä¶ ‚Ä¶
[ You have four actions: add a fiber, delete a fiber,
add 1 Gbps capacity, reduce 1 Gbps capacity ] ‚Ä¶
Step k: add 1 Gbps capacity for link L.
Step k+1: Calculate the total costs.
‚Ä¶ ‚Ä¶
Inference
Cost = ‚àë$%&' ùê∂$%&' √ó ùëêùëúùë†ùë°() + ‚àë*%+,- ùëêùëúùë†ùë°*%+,-
s.t.
ùê∂$%&' ‚â•‚àë*$./‚àà$%&' ùëáùëüùëéùëìùëìùëñùëê(ùëìùëôùëúùë§, ùëôùëñùëõùëò)
and
‚àë$%&'‚àà12345 ùê∂$%&' √ó ùëÜ*%+,-
‚â§ùëÄ*%+,-
from docplex.mp.model import Model
‚Ä¶ ‚Ä¶ mdl = Model() ‚Ä¶ mdl.solve()
[ Incorporate 
human intervention ]
Scripts
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
‚Ä¶ ‚Ä¶
def add_fiber(graph, u, v):
‚Ä¶ ‚Ä¶
def add_capacity(graph, u, v):
‚Ä¶ ‚Ä¶
nx.draw(G_fiber, G_ip)
Heavy-loaded
Light-loaded
IP layer
Fiber layer
‚Ä¶ ‚Ä¶
‚Ä¶ ‚Ä¶
Calculator
Planner
Executor
Results
with Traffic Matrix n
‚Ä¶ ‚Ä¶
with Traffic Matrix 1
‚Ä¶ ‚Ä¶
Link capacity scheme
Examples of final outputs
P = { ùëÉ6, ùëÉ7, ùëÉ" }
Network capacity plan is to design adequate
resources to handle data traffic ‚Ä¶
We can use PuLP or Cplex to solve constraints.
‚Ä¶
In Python, we can use libraries such as NetworkX
for network topology creation and manipulation, 
and Matplotlib or Graphviz for visualization.
Concepts and Tools
Analyzer
‚ë†
‚ë¢
‚ë£
‚ë°
‚ë†
‚ë†
Fig. 2: The ChatNet consists of the analyzer, planner, calculator, and executor, each of which is powered by a network LLM.
Under the case study of network planning, ChatNet is fed with prompts and ultimately outputs diverse capacity schemes.
1) Analyzer: Powered by network LLMs, the analyzer is de-
signed to extract key concepts, tools, and their relationships to
assess the feasibility of network tasks. Generally, the analyzer
is fed with a prompt of natural language descriptions L that
should cover the range of {LT , LS, LC}, where LT is the task
description, LS is the network state, and LC denotes network
constraints. Moreover, there is an additional Ô¨Åle interface for
uploading datasets and linking Ô¨Åles.
2) Planner: The planner reasons out the necessary step-by-
step process to complete the network tasks, where the planning
space P is deÔ¨Åned by sets {PL, PA, PS}. PL depicts the
planning logic, such as simple sequence or loop steps. PA
is a collection of customizable operations and actions, such
as reading Ô¨Åles and accessing tools. PS denotes the skills
that are needed to leverage speciÔ¨Åc network tools. It is worth
noting that the proposed module is serving as plan creation,
rather than plan execution. This means that users can conduct
multiple rounds of dialogue with the planner through the CoT,
and even modify the plan directly.
3) Calculator: LLMs are not good at network mathematic
and formulation, while there are complex numerical calcu-
lations and model constraints in the network system. Thus,
LLMs must have an additional calculation module to compute
parameters for each step. For instance, the calculator can
invoke programming language to implement simple arithmetic
operations, or import solvers to optimize constrained models.
Network LLMs can generate useful scripts based on prompts
to speed up the network modeling process. Some human
intervention is inevitable for the collaboration of the calculator
and the planner considering complex network tasks.
4) Executor: The executor is responsible for outputting
the Ô¨Ånal results. Typically, the executor generates networking
schemes and protocols, as well as network conÔ¨Åguration
commands (e.g., ACL and CLI) by coding. Through uniÔ¨Åed
API of network LLMs, the executor can also be integrated into
network emulators, controllers, and veriÔ¨Åcation tools.
B. Case Study under Network Planning
We simulate a prototype of ChatNet with the support of
GPT-4, where four GPT-4 models are initially prompted to
act as analyzer, planner, calculator, and executor, respectively.
Each model is constructed through a series of prompt engineer-
ing techniques, including role-playing implications, designing
prompt templates, deÔ¨Åning task instructions, adding external
data, limiting output formats, providing example solutions, and
incorporating human-in-the-loop feedback. As illustrated in
Fig. 2, the prompts position the role of analyzer as a network
planner, and then inform it of the trafÔ¨Åc matrices, capacity
constraints, optimization goals, and desired task outputs. x
Firstly, the analyzer explains network capacity planning and
points out the required tools, such as Cplex for constraint
solving, NetworkX for network topology creation, and Mat-
plotlib for visualization. These outputs are manually delivered
to other modules as input prompts. y Then, the planner
module decouples tasks and starts executing them step by step,
which mainly includes reading trafÔ¨Åc matrix Ô¨Åles, calculating
network capacity solutions, and drawing IP and optical net-
work topology. Moreover, personalized actions (e.g., add the
Ô¨Åber or add the capacity) can be designated in the planner to
instruct further modiÔ¨Åcations to the network topology. z The
calculation formula of the cost and optimization model are
stored in the calculator module prior, which is a combination
of scripts provided by ChatNet and human intervention. {
Finally, the executor generates customized network capacity
solutions, such as using colors to display different congestion
This article has been accepted for publication in IEEE Network. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/MNET.2024.3435752
¬© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Tsinghua University. Downloaded on September 02,2024 at 10:50:17 UTC from IEEE Xplore.  Restrictions apply. 

Analyzer
Planner
Calculator
Executor
The components
0.0
0.2
0.4
0.6
0.8
1.0
Relative LLM Score
Zero-shot
Few-shot
CoT
RAG
0
2
4
6
8
10
12
Relative number of HI
Relative HI
Fig. 3: Four prompt methods of zero-shot, few shot, CoT, and
RAG are compared. The relative LLM score and number of
human intervention (HI) are adopted to evaluate the perfor-
mance of the four components of ChatNet.
levels, and using dotted and solid lines to display hierarchical
IP and optical network topologies.
Due to the lack of task benchmarks and instruction datasets
in network domains, we combine the LLM evaluators22 [14]
and expert evaluation [15] to brieÔ¨Çy assess the four compo-
nents under the prompt methods of zero-shot, few shot, CoT,
and RAG. The scoring is normalized to a range of [0, 1]. In
this scale, scores of 0.8, 0.6, and 0.4 indicate that the results
align with expectations to a high, moderate, and just adequate
degree, respectively. A score of 0.2 signiÔ¨Åes that the results
do not match expectations. If automatic prompt delivery can
be achieved between modules, more objective metrics could
be used to evaluate the overall performance of ChatNet, such
as high-level planning (HLP)23 accuracy and the percentage of
completed tasks. As shown in Fig. 3, we Ô¨Ånd that the analyzer
always achieved a score higher than 0.8 with no more than one
count of human intervention (HI), while the calculator is the
bottleneck of the entire framework and requires multiple hu-
man interventions. When the RAG is employed by uploading
a speciÔ¨Åc network planning design document, the calculator is
capable of extracting mathematical constraints and providing a
model that is basiclly correct. Moreover, the CoT can improve
the performance of the planner to some extent.
It is worth noting that LLMs are prone to errors during
the inference process, and currently do not match expert
performance on network planning tasks. To improve the ef-
fectiveness of adapting to varied external solvers, ChatNet
could be pre-trained with corresponding user manuals, relevant
code repositories, and demonstration scripts. Also, reÔ¨Åning
intents with parameter templates could enhance the accuracy
of translating from natural language into solver-speciÔ¨Åc input
formats. Most importantly, LLMs reduce the time required for
22An instance of the LLM evaluator to score the output quality of the
analyzer module is at https://github.com/Hyduni001/ChatNet.
23LLM-Planner: https://osu-nlp-group.github.io/LLM-Planner/
burdensome network planning tasks, which greatly improves
processing efÔ¨Åciency. Building extensive network tasks and
instructions to evaluate a variety of domain-adapted network
LLMs will be future work.
V. CHALLENGES AND FUTURE PROSPECTS
In this section, we analyze the challenges brought by the
domain-adapted network LLMs and highlight the potential
research directions.
A. Training Multi-modal Network LLMs
The integration of diverse data types, such as text, images,
and network-speciÔ¨Åc codes, requires a sophisticated training
process to construct multi-modal LLMs for networking. The
model must be adept at processing and interpreting this hetero-
geneous data in a way that accurately reÔ¨Çects the complexities
of network environments. Moreover, there is an issue of
maintaining model relevance over time. Network technologies
and protocols evolve rapidly, necessitating continuous updates
to the training data. Balancing these factors while minimizing
training costs and computational resources is a signiÔ¨Åcant
challenge, which must be addressed to fully earth the potential
of multi-modal network LLMs.
B. Developing Network LLM Plugins
Network LLMs could be deployed on cloud platforms, edge
servers, or local devices, and embedded into existing network
systems through APIs, ofÔ¨Åcial SDKs, third-party libraries, and
plugins. The development of network LLM plugins opens a
new frontier in network management and design. These plug-
ins are intended to extend the capabilities of LLMs, allowing
them to interact more effectively with various network com-
ponents and systems. The challenge lies in designing plugins
that are both Ô¨Çexible enough to accommodate a wide range of
network architectures and speciÔ¨Åc enough to provide meaning-
ful insights and actions. Interoperability is a key concern, as
these plugins must seamlessly integrate with existing network
management tools and protocols. Additionally, ensuring the
security and reliability of these plugins is paramount.
C. Enabling Network Embodied Intelligence
The realization of network embodied intelligence through
LLMs holds the promise of more responsive, efÔ¨Åcient, and
self-optimizing network systems, representing a signiÔ¨Åcant
leap forward in network Ô¨Åelds. For instance, it would be
meaningful to integrate network LLMs into decision-making
systems, such as the network planning system with the deep re-
inforcement learning. Several techniques, such as model com-
pression (e.g., pruning, quantization), hardware acceleration,
edge deployment, batching, and parallelism, can be leveraged
to decrease the model size, reduce inference time, and enhance
the real-time decision-making capabilities. Furthermore, one
fundamental issue is transparency and explainability, since
LLMs may create fake network designs and conÔ¨Ågurations.
Similar to the intelligence from the human driver to full
automation in autonomous driving, network embodied intel-
ligence needs to be considered in layers, e.g., from manual
This article has been accepted for publication in IEEE Network. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/MNET.2024.3435752
¬© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Tsinghua University. Downloaded on September 02,2024 at 10:50:17 UTC from IEEE Xplore.  Restrictions apply. 

operations to assisting network operators, and ultimately to
completely replacing network experts.
VI. CONCLUSIONS
This article has studied the applications of LLMs for
networking. We summarized the enabling techniques for es-
tablishing domain-adapted network LLMs and analyzed the
prompt engineering of zero-shot, few shot, CoT, and RAG. A
novel ChatNet framework is proposed to exhibit the network
embodied intelligence and a case study has been conducted
under the network capacity planning scenario. Finally, the
challenges, such as training multi-modal network LLMs, and
developing network LLM plugins, are discussed. We hope that
ChatNet can serve as an inspiration for future research.
REFERENCES
[1] B. Tian, X. Zhang, E. Zhai, H. H. Liu, Q. Ye, C. Wang, X. Wu, Z. Ji,
Y. Sang, M. Zhang et al., ‚ÄúSafely and automatically updating in-network
acl conÔ¨Ågurations with intent language,‚Äù in Proceedings of the ACM
SIGCOMM Conference, August 2019, pp. 214‚Äì226.
[2] H. Chen, Y. Miao, L. Chen, H. Sun, H. Xu, L. Liu, G. Zhang, and
W. Wang, ‚ÄúSoftware-deÔ¨Åned network assimilation: Bridging the last mile
towards centralized network conÔ¨Åguration management with nassim,‚Äù in
Proceedings of the ACM SIGCOMM Conference, August 2022, pp. 281‚Äì
297.
[3] L. Pang, C. Yang, D. Chen, Y. Song, and M. Guizani, ‚ÄúA survey on
intent-driven networks,‚Äù in IEEE Access, vol. 8, January 2020, pp.
22 862‚Äì22 873.
[4] L. Bariah, Q. Zhao, H. Zou, Y. Tian, F. Bader, and M. Debbah, ‚ÄúLarge
language models for telecom: The next big thing?‚Äù in arXiv 2306.10249,
June 2023, pp. 1‚Äì7.
[5] Y. Huang, M. Xu, X. Zhang, D. Niyato, Z. Xiong, S. Wang, and
T. Huang, ‚ÄúAI-generated network design: A diffusion model-based
learning approach,‚Äù in IEEE Network, vol. 38, no. 3, May 2024, pp.
202‚Äì209.
[6] Y. Chen, R. Li, Z. Zhao, C. Peng, J. Wu, E. Hossain, and H. Zhang,
‚ÄúNetGPT: A native-ai network architecture beyond provisioning person-
alized generative services,‚Äù in arXiv 2307.06148, July 2023, pp. 1‚Äì10.
[7] Y. Ouyang, C. Yang, Y. Song, X. Mi, and M. Guizani, ‚ÄúA brief survey
and implementation on reÔ¨Ånement for intent-driven networking,‚Äù in
IEEE Network, vol. 35, no. 6, November 2021, pp. 75‚Äì83.
[8] Y. Song, C. Yang, J. Zhang, X. Mi, and D. Niyato, ‚ÄúFull-life cycle
intent-driven network veriÔ¨Åcation: Challenges and approaches,‚Äù in IEEE
Network, vol. 37, no. 5, September 2023, pp. 145‚Äì153.
[9] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, and M. Z. Shou,
‚ÄúAssistGPT: A general multi-modal assistant that can plan, execute,
inspect, and learn,‚Äù in arXiv 2306.08640, June 2023, pp. 1‚Äì21.
[10] A. Maatouk, F. Ayed, N. Piovesan, A. D. Domenico, M. Debbah, and Z.-
Q. Luo, ‚ÄúTeleqna: A benchmark dataset to assess large language models
telecommunications knowledge,‚Äù in arXiv 2310.15051, October 2023,
pp. 1‚Äì9.
[11] Y. Miao, Y. Bai, L. Chen, D. Li, H. Sun, X. Wang, Z. Luo, Y. Ren,
D. Sun, X. Xu, Q. Zhang, C. Xiang, and X. Li, ‚ÄúAn empirical study
of netops capability of pre-trained large language models,‚Äù in arXiv
2309.05557, September 2023, pp. 1‚Äì12.
[12] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V.
Le, D. Zhou et al., ‚ÄúChain-of-thought prompting elicits reasoning in
large language models,‚Äù in Advances in Neural Information Processing
Systems, vol. 35, December 2022, pp. 24 824‚Äì24 837.
[13] M. Xu, P. Huang, W. Yu, S. Liu, X. Zhang, Y. Niu, T. Zhang, F. Xia,
J. Tan, and D. Zhao, ‚ÄúCreative robot tool use with large language
models,‚Äù in arXiv 2310.13065, October 2023, pp. 1‚Äì19.
[14] J. Wang, Y. Liang, F. Meng, Z. Sun, H. Shi, Z. Li, J. Xu, J. Qu, and
J. Zhou, ‚ÄúIs ChatGPT a good NLG evaluator? a preliminary study,‚Äù in
arXiv 2303.04048, March 2023, pp. 1‚Äì11.
[15] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Car-
bune, and A. Rastogi, ‚ÄúRLAIF: Scaling reinforcement learning from
human feedback with AI feedback,‚Äù in arXiv 2309.00267, September
2023, pp. 1‚Äì29.
This article has been accepted for publication in IEEE Network. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/MNET.2024.3435752
¬© 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Tsinghua University. Downloaded on September 02,2024 at 10:50:17 UTC from IEEE Xplore.  Restrictions apply. 

