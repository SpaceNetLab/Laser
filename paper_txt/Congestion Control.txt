Congestion Control in Machine Learning Clusters Sudarsanan Rajasekaran Massachusetts Institute of Technology Manya Ghobadi Massachusetts Institute of Technology Gautam Kumar Google Aditya Akella UT Austin

This paper argues that fair-sharing, the holy grail of congestion control algorithms for decades, is not necessarily a desirable property in Machine Learning (ML) training clusters. We demonstrate that for a speciﬁc combination of jobs, introducing unfairness improves the training time for all competing jobs. We call this speciﬁc combination of jobs compatible and deﬁne the compatibility criterion using a novel geometric abstraction. Our abstraction rolls time around a circle and rotates the communication phases of jobs to identify fully compatible jobs. Using this abstraction, we demonstrate up to 1.3× improvement in the average training iteration time of popular ML models. We advocate that resource management algorithms should take job compatibility on network links into account. We then propose three directions to ameliorate the impact of network congestion in ML training clusters: (𝑖) an adaptively unfair congestion control scheme, (𝑖𝑖) priority queues on switches, and (𝑖𝑖𝑖) precise ﬂow scheduling. CCS CONCEPTS • Networks →Data center networks; Network resources allocation; Transport protocols; Network management; • Computing methodologies →Neural networks; KEYWORDS Congestion control, Networks for ML, Resource allocation, Datacenters for ML, Transport layer, DNN training ACM Reference Format: Sudarsanan Rajasekaran, Manya Ghobadi, Gautam Kumar, and Aditya Akella. 2022. Congestion Control in Machine Learning Clusters. In The 21st ACM Workshop on Hot Topics in Networks (HotNets ’22), November 14–15, 2022, Austin, TX, USA. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3563766.3564115 HotNets’22, November 14–15, 2022, Austin, TX, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9899-2/22/11. https://doi.org/10.1145/3563766.3564115 1

The ever-growing increase in dataset and model sizes of deep neural networks (DNNs) has created a massive demand for efﬁcient GPU clusters. Several studies have demonstrated that as the number of GPUs increases, the communication overhead of distributed Machine Learning (ML) training workloads quickly takes up a signiﬁcant portion of training iteration time . To alleviate the communication overhead of distributed ML training, many training platforms overlap the compute and communication phases of a single job using pipelining , smart memory management , or prioritized parameter transfers . But these approaches tend to consider a job in isolation, and the impact of congestion control algorithms, when two or more training jobs share a bottleneck link is largely ignored. Today’s systems for ML simply attempt to place workers of the same job close to each other to minimize the probability of congesting the network and rely on default TCP or RDMA protocols to handle congestion . For instance, Themis uses a slowdown factor to give preference to place workers of the same job under the same Top-of-Rack (ToR) switch, but it does not take network congestion into account when workers are spread across ToRs. Even with careful job placement, cross-job network contention is inevitable in large-scale GPU clusters. Today, when two or more ML jobs compete for bandwidth, congestion control approaches share the network resources fairly, but we demonstrate that for a speciﬁc combination of jobs, introducing unfairness creates a desirable side effect that improves the training time of all jobs competing for bandwidth (§2). Essentially, unfairness interleaves the computation and communication phases of different jobs, enabling them to claim the network bandwidth one-at-a-time, thereby improving the training time of all competing jobs. We refer to this set of jobs as compatible. However, if jobs are incompatible, unfairness unnecessarily hurts those that are less aggressive. Identifying compatible jobs is challenging because it depends on the duration of compute and communication phases of competing jobs, and their network bandwidth requirements. To address this challenge, 235 5IJTXPSLJTMJDFOTFEVOEFSB$SFBUJWF$PNNPOT"UUSJCVUJPO*OUFSOBUJPOBM-JDFOTF HotNets’22, November 14–15, 2022, Austin, TX, USA Sudarsanan Rajasekaran et al.

! " #

  ! 	     )* !"" "! (  %"! 

! " #

  ! 	     )* !"" "! (  %"!  )*" ""!)  # $!! $ *

 " $ %

  !                         )*# "!"%"# ,++  ! $ !'-+!!'

& !    !   	     Figure 1: Impact of unfairness on training iteration time of two VGG19 training jobs sharing bottleneck link 𝐿1. we propose a geometric abstraction that leverages the periodic on-off pattern of DNN training (§3). The key idea of our abstraction is to roll time around a circle whose perimeter is proportional to the training iteration time of the ML jobs. Using our geometric abstraction, we argue ML cluster operators can manage network congestion by ensuring fully compatible jobs are placed on the same network links, and introducing some form of unfairness to enable cross-job communication/computation interleaving across compatible jobs (§4). Toward this goal, we propose three potential future directions that use our geometric abstraction to alleviate congestion in ML training clusters : (𝑖) an adaptively unfair congestion control scheme, (𝑖𝑖) priority queues on switches, and (𝑖𝑖𝑖) precise ﬂow scheduling. 2 SURPRISING PAYOFF OF UNFAIRNESS

large DNN models is data parallelism, where the training data is distributed across multiple accelerators. During each training iteration, accelerators need to synchronize their model weights. This step is called allreduce and can be performed using various techniques, such as broadcasting , parameter servers , ring-allreduce , tree-reduce , or hierarchical ring-allreduce . Compute and communication phases. To consider the impact of network congestion in data parallel training jobs, we refer to the forward pass as the compute phase and refer to the backpropagation and allreduce phases together as the communication phase because congestion impacts any period of time when data is being injected into the network. Pipeline parallelism. To speed up training, many platforms pipeline the computation in the backpropagation step with the communication in the allreduce step . Pipelining techniques are effective at overlapping the computation and communication phases of the same job, but they ignore the interaction between multiple jobs when they share a bottleneck link. Goal. We consider GPU training clusters where large DNN models are distributed across GPUs. Our ultimate goal is to avoid network congestion to slow down the training time of jobs sharing a link. We argue that achieving this goal does not always require augmenting the network bandwidth, and compatible jobs can share network links without experiencing any slowdowns, as if the jobs are running with dedicated network resources. A surprising observation. We begin our argument with an observation from a testbed with A100 GPUs and ConnectX-5 Mellanox NICs, shown in Figure 1a. The capacity of each NIC is 50 Gbps. We run two distributed DNN training jobs, called 𝐽1 and 𝐽2, across the servers such that they share a bottleneck link 𝐿1. We run each job for 1,000 iterations under two scenarios. In the ﬁrst scenario, two VGG19  training jobs start simultaneously and share 𝐿1 fairly under the default RDMA-based DCQCN congestion control algorithm . DCQCN has a parameter𝑇that corresponds to the time period of its rate increase. The default value of 𝑇in our testbed is 125 𝜇s. Figure 1b shows both jobs achieve roughly 21 Gbps bandwidth (i.e., half of 𝐿1’s capacity) during the ﬁrst iteration. This is not surprising, as DCQCN is designed to divide the capacity equally between jobs . In the second scenario, we artiﬁcially introduce unfairness by adjusting DCQCN’s 𝑇 parameter on 𝐽1’s servers to 100 𝜇s, making it more aggressive. As a result, in the very ﬁrst iteration, 𝐽1 achieves roughly 30 Gbps of bandwidth, whereas 𝐽2 achieves 15 Gbps, as shown in Figure 1c. At ﬁrst blush, it appears continuous unfairness will hurt 𝐽2’s iteration time in subsequent iterations. But we ﬁnd that as training progresses, unfairness helps both 𝐽1 and 𝐽2. Figure 1d plots the CDF of training iteration times for both scenarios, demonstrating that the unfairness in scenario2 accelerates the median iteration time of both jobs by 1.23×. Why would unfairness in congestion control help ML training? DNN training jobs have a unique on-off pattern  where the off period corresponds to the compute phase and the on period represents the communication phase. 236 Congestion Control in Machine Learning Clusters HotNets’22, November 14–15, 2022, Austin, TX, USA ) *) +) ,) -) .) /) 0) ) )$* )$+ )$, )$)$. )$/ )$0 )$1 )$2 * *$* *$+ *$, *$*$. 	# %& 	   	   ) *) +) ,) -) .) /) 0) ) )$* )$+ )$, )$)$. )$/ )$0 )$1 )$2 * *$* *$+ *$, *$*$. 	# %& 	   	    $   $   $   $   $   $   $   $      *      

   *  +  ,      "  $  !    $ +      "      "      $ $    $      "   $   $   $   $   .      " Figure 2: Comparing the iteration times with fair and unfair bandwidth allocations. Intuitively, when two training jobs share a network link, fair bandwidth sharing slows down both jobs by prolonging their communication phases. In contrast, unfair bandwidth sharing speeds up one job while slowing down the other, creating a side effect that slides the on-off pattern of the two competing jobs to ﬁt into each other after a few iterations. Figure 2 demonstrates the sliding impact for the two scenarios in Figure 1 by showing the link utilization of back-to-back iterations. For clarity of presentation, we assume both jobs start at the same time, and we smooth out the plots to help with the visualization. Figure 2a shows that when the bottleneck link is shared fairly, both jobs continue to occupy ≈50% of the available bandwidth across all iterations. In contrast, Figure 2b shows that for the very ﬁrst iteration, unfairness gives more bandwidth to 𝐽1, allowing it to accelerate and complete the ﬁrst iteration at 𝑡= 0.28 sec, whereas 𝐽2 takes longer and completes its ﬁrst iteration at 𝑡= 0.32 sec. This imbalance means the second communication phase of 𝐽1 starts earlier (at 𝑡= 0.38 sec) and utilizes the full bandwidth temporarily, while the second communication phase of 𝐽2 starts later (at 𝑡= 0.42 sec). Similarly, in the second iteration, when both jobs are communicating, due to unfairness, 𝐽1 occupies a bigger share of the bandwidth. Interestingly, the region where both jobs compete for network communication gradually reduces as we move from the ﬁrst iteration to the fourth iteration. By the fourth iteration, unfairness pulls apart the communication phases of the jobs and interleaves the computation phase of 𝐽1 with the communication phase of 𝐽2 perpetually. Hence, the iteration times of both jobs become almost equal Jobs competing for bandwidth (batch size) Fairness iter. time Unfairness iter. time Unfairness speed-up Fully compatible BERT (8) 183 ms 157 ms 1.17×  VGG19 (1200) 297 ms 315 ms 0.94× DLRM (2000) 1301 ms 1001 ms 1.3×  DLRM (2000) 1300 ms 1019 ms 1.28× BERT (8) 320 ms 216 ms 1.48× VGG19 (1400) 494 ms 466 ms 1.06×  WideResNet (800) 466 ms 505 ms 0.92× WideResNet (800) 295 ms 273 ms 1.08×  VGG16 (1400) 294 ms 274 ms 1.07× VGG19 (1400) 389 ms 329 ms 1.18× VGG16 (1700) 389 ms 329 ms 1.18×  ResNet50 (1600) 167 ms 165 ms 1.01× Table 1: Fully compatible jobs are those for which unfairness speeds up all the jobs in the group (colored in green). to what they would have been had the jobs been running in a dedicated cluster; i.e., faster than fair sharing. Is unfairness helpful for all ML jobs? It turns out that the above desirable side effect of unfair bandwidth sharing can only help jobs whose communication and computation phases can ﬁt perfectly into each other. As Table 1 shows, unfairness only helps a speciﬁc combination of jobs. Each row represents a different group of popular DNN training jobs (and batch sizes). We ﬁrst measure the average iteration time when each group of jobs competes for bandwidth using the default (fair) DCQCN algorithm. Then, for each group of jobs, we make the DCQCN algorithm unfair such that the order of aggressiveness is based on the jobs’ order of appearance in the table, with each job more aggressive than subsequent jobs in its row. The ﬁrst group shows that when BERT and VGG19  models share a link, making BERT more aggressive ends up negatively hurting the iteration time of 237 HotNets’22, November 14–15, 2022, Austin, TX, USA Sudarsanan Rajasekaran et al.               

			  	 		

    	           

			  	     	

Figure 3: Our geometric abstraction. 

  	  

 

  	 

     Figure 4: Jobs 𝐽1 and 𝐽2 are compatible. VGG19. But the second group shows when two DLRM models share a link, making the ﬁrst DLRM more aggressive accelerates the average iteration time of both jobs by 1.28 × −1.3×, compared to fair bandwidth sharing. The green color indicates the set of jobs for which unfairness leads to faster iteration times than fair bandwidth sharing. We refer to a

ulers, including BytePS , Themis , Pollux , and Muri . Creating unfairness for compatible jobs. After placing compatible jobs on network links, the next step is to avoid 1The details of the optimization formulation are omitted for brevity. colliding the communication phases of jobs, so that they can co-exist on the same link while using the link bandwidth one at a time. This property can be achieved by: (𝑖) deploying an unfair congestion control algorithm; or (𝑖𝑖) using packet priorities to achieve unfairness; or (𝑖𝑖𝑖) scheduling the communication phases in the appropriate time slots. Below we

Using an unfair transport protocol. Intuitively, deploying an unfair congestion control algorithm throughout a cluster seems like a bad idea. In particular, as shown in Table 1, when incompatible jobs share a link, our unfair transport protocol favors more aggressive jobs and slows down the less aggressive ones without creating any desirable side effects. However, we argue that an adaptively unfair congestion control algorithm can achieve the desired side effect of unfairness when the jobs are compatible without negatively impacting incompatible jobs. For instance, the DCQCN algorithm uses the following equation to determine the increase in its target sending rate: 𝑅𝑇= 𝑅𝑇+ 𝑅𝐴𝐼, where 𝑅𝑇represents the target sending rate, and 𝑅𝐴𝐼denotes the Additive Increase step. To enable adaptive unfairness, we adjust 𝑅𝐴𝐼from a constant to 𝑅𝐴𝐼(1 + 𝐷𝑎𝑡𝑎𝑠𝑒𝑛𝑡 𝐷𝑎𝑡𝑎𝑐𝑜𝑚𝑚. 𝑝ℎ𝑎𝑠𝑒), where 𝐷𝑎𝑡𝑎𝑠𝑒𝑛𝑡represents the amount of progress in the communication phase. Hence, a job closer to completing its communication phase is more aggressive than a job just about to start its communication phase (𝐷𝑎𝑡𝑎𝑠𝑒𝑛𝑡= 0), enabling interleaving of compatible jobs. Meanwhile, incompatible jobs continue to take turns in becoming the aggressive party to claim the bandwidth, and they end up sharing the bandwidth fairly in steady state.

ilar to prior techniques using application-aware semantics in datacenters to achieve differential performance . For ML workloads, the assigned priority for jobs can be arbitrary as long as the jobs competing for the same link are compatible and have a unique priority. In this

sharing the same link. Then, the end-hosts mark packets with the assigned priority, allowing the switch  to divide the link bandwidth accordingly, thereby mimicking the desirany changes to the congestion control algorithm. However, a potential challenge is that today’s switches support a few priority queues; thus, maintaining unique priorities when there is a large number of jobs becomes challenging. Flow scheduling. Instead of creating explicit unfairness in the congestion control algorithm, service providers can use the centralized scheduler to schedule ﬂows for each job at precise time intervals. Concretely, the output of our optimization formulation provides an angle of rotation for each job such that the communication phases do not collide. This angle 239 HotNets’22, November 14–15, 2022, Austin, TX, USA Sudarsanan Rajasekaran et al. corresponds to a time-shift for the communication phase of a job. Using this time-shift, the scheduler can schedule ﬂows at appropriate times to avoid colliding the communication

lar to ﬂow-scheduling techniques in datacenters . However, it is challenging to schedule short transfers at precise times without a high-resolution clock synchronization across the cluster. 5

the loss. Scheduling techniques. DNN scheduling algorithms decide where jobs are placed in the cluster. Today’s schedulers, such as Gandiva , Themis , Pollux , Tiresias , ByteScheduler , and Optimus , try to minimize congestion by placing workers of the same job as close to each other as possible. But these approaches do not consider placing compatible jobs on network links to avoid sharing the network bandwidth. Flow-scheduling approaches such as Sincronia , Orchestra , and Coﬂows provide differential treatment for ﬂows instead of sharing the network fairly. But these approaches are designed for legacy datacenter trafﬁc and do not leverage the periodic behavior of

ML workloads. Muri  and Synergy  recently proposed DL scheduling techniques to interleave critical resources (e.g., GPU, CPU, network, storage) of DL training jobs. However, Muri considered a restrictive setting where resource interleaving is only possible for jobs distributed across the exact same set of servers, and Synergy only considered GPU, CPU,

more generic case where network links are shared across jobs, irrespective of the set of GPUs they use. 7

time for all the jobs in the group as compatible jobs. 3 GEOMETRIC ABSTRACTION To determine whether a set of jobs competing on a link is compatible, we seek to answer the following question: “Is there a way to slide the communication pattern of the jobs such that their communication phases have almost no overlap with each other? Roll time around a circle. To answer above question, we propose a novel geometric abstraction. Consider the timeseries representation of the network demand for a job running in a dedicated cluster with no congestion. Given the periodic on-off pattern of DNN training, the duration of the compute and communication phases remains more or less the same across training iterations. Consequently, if we roll time around a circle whose perimeter is equal to the training iteration time, the communication phases of all iterations will appear approximately on the same arc of the circle. For instance, Figure 3a illustrates the time-series network demand of VGG16 with a training iteration time of 255 ms where the ﬁrst 141 ms are pure computation (i.e., forward pass). Figure 3b shows a circle with perimeter 255 time units and the time-series data plotted around it in a counter-clockwise direction. The compute phase of all iterations occupies the arc starting at 0 and ending at 141 time units, and the communication phases span the rest of the circle. This representation demonstrates that the compute and communication phases of different iterations always cover the same arcs of the circle. We design our geometric abstraction to capture this circular property. Figure 3c shows our geometric abstraction. The circle’s perimeter represents the iteration time, set to 255 time units. The compute phase spans 141 time units, represented by the uncolored arc, and the communication phase, represented by the colored arc, occupies the remainder of the circle. Rotate the circle to avoid congestion. To determine the compatibility of two (or more) jobs, we place each job on its corresponding circle and overlay the circles on top of each other. Congestion happens when the communication phases collide, as shown in Figure 4a. To avoid congestion, we rotate the circles to ﬁnd a position where the communication arcs do not collide, as shown in Figure 4b. If such a rotation is found, the jobs are deemed compatible. Note that rotating the circles clockwise and counterclockwise is equivalent to the sliding effect of unfairness. Moreover, overlapping the computation times of each job (the uncolored regions) is acceptable, as we assume jobs are not sharing the compute resources with each other. How to capture jobs with different iteration times? The above technique is only applicable when circles have the same perimeter. To generalize our geometric abstraction to the case where jobs have different iteration times, we place each job on a uniﬁed circle whose perimeter is equal to the Least Common Multiple (LCM) of the iteration times of all jobs competing on the link. For instance, consider two jobs 𝐽1 and 𝐽2 competing on a bottleneck link with iteration times 40 ms and 60 ms, respectively. To determine the compatibility of these two jobs, we place them on a circle with a perimeter equal to 𝐿𝐶𝑀(40, 60) = 120 units. Figure 5a shows 𝐽1 on this uniﬁed circle. Given that the perimeter of the circle is 3× 𝐽1’s iteration time, there are three communication and computation phases in the ﬁgure. Similarly, Figure 5b shows 𝐽2 on the uniﬁed circle. We then overlay the uniﬁed circles (shown in Figure 5c) and rotate them to determine whether the jobs are compatible. Figure 5d shows that when 𝐽1 is rotated 30◦counterclockwise, the colored areas on the circles do not collide; i.e., the jobs are fully compatible. Optimization formulation. We use an optimization formulation to determine whether a set of jobs is fully compatible and if so, what the best angle of rotation is for each job such that the communication phases do not overlap. Our formulation searches for rotation angles such that there is no region on the uniﬁed circle where more than one job is communicating. For scalability, we discretize the circle into small sectors and add constraints capping the number of jobs 238 Congestion Control in Machine Learning Clusters HotNets’22, November 14–15, 2022, Austin, TX, USA 	 	

 	   

   

    			   $  ! $       Figure 5: Geometric abstraction for jobs with different training iteration times using a uniﬁed circle. communicating in each sector at one. If the optimization formulation ﬁnds the rotation angles satisfying the constraints, the jobs are deemed compatible.1 4 CONGESTION-FREE ML CLUSTERS Today’s ML cluster scheduling techniques consider end-hosts that are topologically near each other as the main criterion for reducing network congestion . This section argues for two requirements to move toward congestion-free ML clusters. First, ML schedulers should be augmented to take compatibility into account and to place compatible jobs on network links. In other words, the problem of job placement should be related not only to available resources on servers but also to compatibility on links. Second, once compatible jobs are placed on the same link(s), service providers need to artiﬁcially create the desirable side effect of unfairness to enable compatible jobs to occupy the entire link bandwidth without slowing each other down. Placing compatible jobs on links. To place compatible jobs on network links, the ML scheduler should ﬁrst proﬁle each ML training job in isolation to measure its iteration time, communication pattern, and bandwidth demand for different hyper-parameters. Next, the scheduling algorithm should become aware of network routes (e.g., ECMP routing decisions) for each job. Once the routes are known, the scheduler runs our optimization formulation for the set of jobs placed on a network link to determine their compatibility. If the jobs are incompatible, the scheduler should look for alternative placeUSENIX Association. S. H. Hashemi, S. Abdu Jyothi, and R. Campbell. Tictac: Accelerating distributed deep learning with communication scheduling. In A. Talwalkar, V. Smith, and M. Zaharia, editors, Proceedings of Machine Learning and Systems, volume 1, pages 418–430, 2019. Y. Huang, Y. Cheng, D. Chen, H. Lee, J. Ngiam, Q. V. Le, and Z. Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism. NeurIPS, 2019. A. Jayarajan, J. Wei, G. Gibson, A. Fedorova, and G. Pekhimenko. Priority-based parameter propagation for distributed dnn training. In A. Talwalkar, V. Smith, and M. Zaharia, editors, Proceedings of Machine Learning and Systems, volume 1, pages 132–145, 2019. X. Jia, S. Song, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie, Z. Guo, Y. Yang, L. Yu, T. Chen, G. Hu, S. Shi, and X. Chu. Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes. CoRR, abs/1807.11205, 2018. Y. Jiang, Y. Zhu, C. Lan, B. Yi, Y. Cui, and C. Guo. A uniﬁed architecture for accelerating distributed DNN training in heterogeneous gpu/cpu clusters. In 14th USENIX Symposium on Operating Systems Design

Nov. 2020. M. Khani, M. Ghobadi, M. Alizadeh, Z. Zhu, M. Glick, K. Bergman, A. Vahdat, B. Klenk, and E. Ebrahimi. Sip-ml: High-bandwidth optical network interconnects for machine learning training. In Proceedings of the 2021 ACM SIGCOMM 2021 Conference, SIGCOMM ’21, pages 657–675, New York, NY, USA, 2021. Association for Computing Machinery. M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su. Scaling distributed machine learning with the parameter server. OSDI’14, pages 583–598. USENIX Association, 2014. K. Mahajan, A. Balasubramanian, A. Singhvi, S. Venkataraman, A. Akella, A. Phanishayee, and S. Chawla. Themis: Fair and efﬁcient GPU cluster scheduling. In 17th USENIX Symposium on Networked

Clara, CA, Feb. 2020. USENIX Association. R. Mittal, A. Shpiner, A. Panda, E. Zahavi, A. Krishnamurthy, S. Ratnasamy, and S. Shenker. Revisiting network support for rdma. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, SIGCOMM ’18, page 313–326, New York, NY, USA, 2018. Association for Computing Machinery. J. Mohan, A. Phanishayee, J. J. Kulkarni, and V. Chidambaram. Looking beyond gpus for dnn scheduling on multi-tenant clusters. In USENIX

2022), July 2022. D. Mudigere, Y. Hao, J. Huang, Z. Jia, A. Tulloch, S. Sridharan, X. Liu, M. Ozdal, J. Nie, J. Park, L. Luo, J. A. Yang, L. Gao, D. Ivchenko, A. Basant, Y. Hu, J. Yang, E. K. Ardestani, X. Wang, R. Komuravelli, C.-H. Chu, S. Yilmaz, H. Li, J. Qian, Z. Feng, Y. Ma, J. Yang, E. Wen, H. Li, L. Yang, C. Sun, W. Zhao, D. Melts, K. Dhulipala, K. Kishore, T. Graf, A. Eisenman, K. K. Matam, A. Gangidi, G. J. Chen, M. Krishnan, A. Nayak, K. Nair, B. Muthiah, M. khorashadi, P. Bhattacharya, P. Lapukhov, M. Naumov, L. Qiao, M. Smelyanskiy, B. Jia, and V. Rao. Software-hardware co-design for fast and scalable training of deep learning recommendation models, 2021. D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP’19, pages 1–15, New York, NY, USA, 2019. Association for Computing Machinery. Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo. Optimus: An efﬁcient dynamic resource scheduler for deep learning clusters. In Proceedings 241 HotNets’22, November 14–15, 2022, Austin, TX, USA Sudarsanan Rajasekaran et al. of the Thirteenth EuroSys Conference, EuroSys ’18, New York, NY, USA, 2018. Association for Computing Machinery. Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo. A generic communication scheduler for distributed dnn training acceleration. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP ’19, page 16–29, New York, NY, USA, 2019. Association for Computing Machinery. J. Perry, A. Ousterhout, H. Balakrishnan, D. Shah, and H. Fugal. Fastpass: A centralized "zero-queue" datacenter network. In Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM ’14, page 307–318, New York, NY, USA, 2014. Association for Computing Machinery. A. Qiao, S. K. Choe, S. J. Subramanya, W. Neiswanger, Q. Ho, H. Zhang, G. R. Ganger, and E. P. Xing. Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning. In 15th USENIX

21), pages 1–18. USENIX Association, July 2021. J. R. Quinlan. Induction of decision trees. Mach. Learn., 1(1):81–106, Mar. 1986. S. Ramabhadran and J. Pasquale. Stratiﬁed round robin: A low complexity packet scheduler with bandwidth fairness and bounded delay. In Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications, SIGCOMM ’03, page 239–250, New York, NY, USA, 2003. Association for Computing Machinery. S. Rashidi, M. Denton, S. Sridharan, S. Srinivasan, A. Suresh, J. Nie, and T. Krishna. Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms, page 540–553. IEEE Press, 2021. A. Sergeev and M. D. Balso. Horovod: fast and easy distributed deep learning in tensorﬂow. CoRR, abs/1802.05799, 2018. M. Shaﬁee and J. Ghaderi. Scheduling coﬂows in datacenter networks: Improved bound for total weighted completion time. In Proceedings of the 2017 ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems, SIGMETRICS ’17 Abstracts, page 29–30, New York, NY, USA, 2017. Association for Computing Machinery. M. Shaﬁee and J. Ghaderi. Scheduling coﬂows with dependency graph. IEEE/ACM Trans. Netw., 30(1):450–463, feb 2022. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition, 2015. A. Sivaraman, S. Subramanian, M. Alizadeh, S. Chole, S.-T. Chuang, A. Agrawal, H. Balakrishnan, T. Edsall, S. Katti, and N. McKeown. Programmable packet scheduling at line rate. In Proceedings of the 2016 ACM SIGCOMM Conference, SIGCOMM ’16, page 44–57, New York, NY, USA, 2016. Association for Computing Machinery. P. Taheri, D. Menikkumbura, E. Vanini, S. Fahmy, P. Eugster, and T. Edsall. RoCC: Robust Congestion Control for RDMA, page 17–30. Association for Computing Machinery, New York, NY, USA, 2020. R. Thakur, R. Rabenseifner, and W. Gropp. Optimization of collective communication operations in mpich. Int. J. High Perform. Comput. Appl., 19(1):49–66, Feb. 2005. R. Thakur, R. Rabenseifner, and W. Gropp. Optimization of collective communication operations in mpich. Int. J. High Perform. Comput. Appl., 19(1):49–66, Feb. 2005. Y. Ueno and R. Yokota. Exhaustive study of hierarchical allreduce patterns for large messages between gpus. In 2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), pages 430–439, 2019. B. C. Vattikonda, G. Porter, A. Vahdat, and A. C. Snoeren. Practical tdma for datacenter ethernet. In Proceedings of the 7th ACM European Conference on Computer Systems, EuroSys ’12, page 225–238, New York, NY, USA, 2012. Association for Computing Machinery. W. Wang, M. Khazraee, Z. Zhong, M. Ghobadi, Z. Jia, D. Mudigere, Y. Zhang, and A. Kewitsch. Topoopt: Co-optimizing network topology and parallelization strategy for distributed training jobs. In 20th

(NSDI 23), 2023. Q. Weng, W. Xiao, Y. Yu, W. Wang, C. Wang, J. He, Y. Li, L. Zhang, W. Lin, and Y. Ding. MLaaS in the wild: Workload analysis and scheduling in Large-Scale heterogeneous GPU clusters. In 19th USENIX

22), pages 945–960, Renton, WA, Apr. 2022. USENIX Association. J. Xia, G. Zeng, J. Zhang, W. Wang, W. Bai, J. Jiang, and K. Chen. Rethinking transport layer design for distributed machine learning. In Proceedings of the 3rd Asia-Paciﬁc Workshop on Networking 2019, APNet ’19, page 22–28, New York, NY, USA, 2019. Association for Computing Machinery. W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P. Patel, X. Peng, H. Zhao, Q. Zhang, F. Yang, and L. Zhou. Gandiva: Introspective cluster scheduling for deep learning. In 13th USENIX

18), pages 595–610, Carlsbad, CA, Oct. 2018. USENIX Association. W. Xiao, S. Ren, Y. Li, Y. Zhang, P. Hou, Z. Li, Y. Feng, W. Lin, and Y. Jia. AntMan: Dynamic scaling on GPU clusters for deep learning. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), pages 533–548. USENIX Association, Nov. 2020. P. Xie, J. K. Kim, Y. Zhou, Q. Ho, A. Kumar, Y. Yu, and E. Xing. Lighter-communication distributed machine learning via sufﬁcient factor broadcasting. In Proceedings of the Thirty-Second Conference on Uncertainty in Artiﬁcial Intelligence, pages 795–804, Arlington, Virginia, USA, 2016. AUAI Press. H. Zhang, L. Chen, B. Yi, K. Chen, M. Chowdhury, and Y. Geng. Coda: Toward automatically identifying and scheduling coﬂows in the dark. In Proceedings of the 2016 ACM SIGCOMM Conference, SIGCOMM ’16, page 160–173, New York, NY, USA, 2016. Association for Computing Machinery. L. Zhang. Virtualclock: A new trafﬁc control algorithm for packetswitched networks. ACM Trans. Comput. Syst., 9(2):101–124, may 1991. Y. Zhao, Y. Liu, Y. Peng, Y. Zhu, X. Liu, and X. Jin. Multi-resource interleaving for deep learning training. In Proceedings of the ACM SIGCOMM 2022 Conference, SIGCOMM ’22, page 428–440, New York, NY, USA, 2022. Association for Computing Machinery. Y. Zhu, H. Eran, D. Firestone, C. Guo, M. Lipshteyn, Y. Liron, J. Padhye, S. Raindel, M. H. Yahia, and M. Zhang. Congestion control for largescale rdma deployments. In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, SIGCOMM ’15, page 523–536, New York, NY, USA, 2015. Association for Computing Machinery. Y. Zhu, M. Ghobadi, V. Misra, and J. Padhye. Ecn or delay: Lessons learnt from analysis of dcqcn and timely. In CoNEXT’16, September 2016. 242

Cluster-level compatibility. In large-scale clusters, jobs are likely to traverse multiple links, and they may compete with different jobs on different links. Given the interdependence of all servers participating in a training job, service providers must ensure compatibility is preserved across all links. A potential solution to address this is to expand the perimeter of the uniﬁed circle to become the LCM of the iteration times of the jobs sharing at least one link with other jobs and solving the optimization formulation to ﬁnd a unique rotation angle for each job. GPU multi-tenancy. For simplicity, we assume GPUs are dedicated resources for each job, and different jobs do not share the same GPU – this is not far from how many production clusters run today to ensure predictable and high throughput training performance. Thus, our geometric abstraction only considers the network links as shared resources and allows the compute phases of different jobs to overlap. Recent proposals demonstrate the feasibility of multi-tenancy on GPUs . We note that capturing GPU multi-tenancy is possible by adding more constraints in our optimization formulation, but we omit the details for brevity. Impact of hyper-parameters. The iteration time and communication demand of a job are affected by the batch size, the number of workers, and the all-reduce algorithm. If these hyper-parameters are changed during the lifetime of a job, its geometric abstraction changes accordingly, and the scheduler should take the new abstraction into account. This also provides an opportunity for the scheduler to adjust the hyperparameters to improve the compatibility of jobs sharing links while making job placement decisions. 6

We present a surprising ﬁnding: unfair bandwidth allocation helps certain combinations of DNN jobs achieve congestionfree performance even though the network is shared. We formalize our ﬁnding by deﬁning a novel geometric abstraction to capture job compatibility and argue ML schedulers should use an optimization formulation to take job compatibility into account. We discuss potential approaches to systematically unlock this opportunity to optimize network sharing for compatible jobs. Acknowledgments. We would like to thank HotNets’ anonymous reviewers for their valuable feedback. Special thanks to Zhizhen Zhong for help with the optimization formulation, Weiyang Wang for help with the testbed, Mingran Yang for help with the Toﬁno switch, Benoit Pit–Claudel, Pantea Karimi, Dousabel Tay, Kapil Vaidya, Nandita Dukkipati, and Amin Vahdat for giving valuable suggestions. The MITafﬁliated authors are supported by ARPA-E ENLITENED PINE DE-AR0000843, DARPA FastNICs 4202290027, NSF CNS-2008624, NSF SHF-2107244, NSF ASCENT-2023468, NSF CAREER-2144766, NSF PPoSS-2217099, NSF CNS2211382, Meta faculty award, Google faculty award, and Sloan fellowship FG-2022-18504. The UT Austin author is supported by NSF CNS-2105890, NSF CNS-2106199 and gifts from Meta and Cisco. 240 Congestion Control in Machine Learning Clusters HotNets’22, November 14–15, 2022, Austin, TX, USA

Congestion control for ML. RDMA is currently the standard technology used in ML clusters. Congestion control algorithms for RDMA include DCQCN , IRN , and RoCC . These schemes strive to achieve fairness across all ﬂows sharing a link and do not leverage the unique properties of ML workloads, such as periodicity and predictable network demand. Xia et al.  leverage the loss-tolerance of ML training and propose a bounded-loss tolerant transport as a new congestion control paradigm for ML training workBaidu, 2017. https://github.com/baidu-research/baidu-allreduce. Deep Learning Recommendation Model for Personalization and Recommendation Systems, 2021. https://github.com/facebookresearch/dlrm. S. Agarwal, S. Rajakrishnan, A. Narayan, R. Agarwal, D. Shmoys, and A. Vahdat. Sincronia: Near-optimal network design for coﬂows. SIGCOMM ’18, page 16–29, New York, NY, USA, 2018. Association for Computing Machinery. M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker. pfabric: Minimal near-optimal datacenter transport. In Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM, SIGCOMM ’13, pages 435–446, New York, NY, USA, 2013. ACM. R. Ausavarungnirun, V. Miller, J. Landgraf, S. Ghose, J. Gandhi, A. Jog, C. J. Rossbach, and O. Mutlu. Mask: Redesigning the gpu memory hierarchy to support multi-application concurrency. SIGPLAN Not., 53(2):503–518, mar 2018. M. A. Chang, A. Panda, D. Bottini, L. Jian, P. Kumar, and S. Shenker. Network evolution for dnns. SysML, 2018. M. Chowdhury and I. Stoica. Coﬂow: A networking abstraction for cluster applications. In Proceedings of the 11th ACM Workshop on Hot Topics in Networks, HotNets-XI, page 31–36, New York, NY, USA, 2012. Association for Computing Machinery. M. Chowdhury and I. Stoica. Efﬁcient coﬂow scheduling without prior knowledge. In Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, SIGCOMM ’15, page 393–406, New York, NY, USA, 2015. Association for Computing Machinery. M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and I. Stoica. Managing data transfers in computer clusters with orchestra. In Proceedings of the ACM SIGCOMM 2011 Conference, SIGCOMM ’11, page 98–109, New York, NY, USA, 2011. Association for Computing Machinery. M. Chowdhury, Y. Zhong, and I. Stoica. Efﬁcient coﬂow scheduling with varys. In Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM ’14, page 443–454, New York, NY, USA, 2014. Association for Computing Machinery. E. Chung, J. Fowers, K. Ovtcharov, M. Papamichael, A. Caulﬁeld, T. Massengil, M. Liu, D. Lo, S. Alkalay, and M. Haselman. Accelerating persistent neural networks at datacenter scale. In Hot Chips, volume 29, 2017. R. Cruz. Quality of service guarantees in virtual circuit switched networks. IEEE Journal on Selected Areas in Communications, 13(6):1048–1056, 1995. R. L. Cruz. Service burstiness and dynamic burstiness measures: A framework. J. High Speed Netw., 1(2):105–127, apr 1992. A. Demers, S. Keshav, and S. Shenker. Analysis and simulation of a fair queueing algorithm. SIGCOMM Comput. Commun. Rev., 19(4):1–12, aug 1989. J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. N. Figueira and J. Pasquale. Rate-function scheduling. In Proceedings of INFOCOM ’97, volume 3, pages 1063–1071 vol.3, 1997. N. R. Figueira and J. Pasquale. Leave-in-time: A new service discipline for real-time communications in a packet-switching network. In Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication, SIGCOMM ’95, page 207–218, New York, NY, USA, 1995. Association for Computing Machinery. J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo. Tiresias: A GPU cluster manager for distributed deep learning. In 16th USENIX Symposium on Networked Systems Design