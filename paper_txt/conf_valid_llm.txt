Configuration Validation with Large Language Models
Xinyu Lian, Yinfang Chen, Runxiang Cheng, Jie Huang, Parth Thakkar†, Minjia Zhang, Tianyin Xu
University of Illinois at Urbana-Champaign †Meta Platforms, Inc.
Abstract—Misconfigurations are major causes of software
failures. Existing practices rely on developer-written rules or test
cases to validate configurations, which are expensive. Machine
learning (ML) for configuration validation is considered a
promising direction, but has been facing challenges such as the
need of large-scale field data and system-specific models. Recent
advances in Large Language Models (LLMs) show promise in
addressing some of the long-lasting limitations of ML-based
configuration validation. We present a first analysis on the
feasibility and effectiveness of using LLMs for configuration
validation. We empirically evaluate LLMs as configuration valida-
tors by developing a generic LLM-based configuration validation
framework, named Ciri. Ciri employs effective prompt engineering
with few-shot learning based on both valid configuration and
misconfiguration data. Ciri checks outputs from LLMs when
producing results, addressing hallucination and nondeterminism of
LLMs. We evaluate Ciri’s validation effectiveness on eight popular
LLMs using configuration data of ten widely deployed open-source
systems. Our analysis (1) confirms the potential of using LLMs
for configuration validation, (2) explores design space of LLM-
based validators like Ciri, and (3) reveals open challenges such
as ineffectiveness in detecting certain types of misconfigurations
and biases towards popular configuration parameters.
I. INTRODUCTION
Modern software systems undertake hundreds to thousands
of configuration changes on a daily basis [11], [13], [45], [46],
[67], [68], [70]. For example, at Meta/Facebook, thousands of
configuration file “diffs” are committed daily, outpacing the
frequency of code changes [45], [70]. Other systems such as
at Google and Microsoft also frequently deploy configuration
changes [11], [13], [46]. Such velocity of configuration changes
inevitably leads to misconfigurations. Today, misconfigurations
are among the dominating causes of production incidents [11],
[26], [33], [45], [55], [61], [67], [89], [90].
To detect misconfigurations, today’s configuration manage-
ment systems employ the “configuration-as-code” paradigm and
enforce continuous configuration validation, ranging from static
validation, to configuration testing, and to manual review and
approval [70]. The configuration is first checked by validation
code (aka validators) based on predefined correctness rules [12],
[29], [36], [41], [50], [58], [70], [91]; in practice, validators are
written by engineers [12], [58], [70]. After passing validators,
configuration changes are then tested with code to check
program behavior [69], [87]. Lastly, configuration changes
are reviewed like source-code changes.
The aforementioned pipeline either relies on manual in-
spection to spot misconfigurations in the configuration file
diffs, or requires significant engineering efforts to implement
and maintain validators or test cases. However, these efforts
are known to be costly and incomprehensive. For example,
despite that mature projects all include extensive configuration
validators, recent work [32], [38], [39], [74], [75], [86], [88]
repeatedly shows that existing validators are insufficient. The
reasons are twofold. First, with large-scale systems exposing
hundreds to thousands of configuration parameters [85], imple-
menting validators for every parameter becomes a significant
overhead. Recent studies [70], [88] report that many parameters
are uncovered by existing validators, even in mature software
projects. Second, it is non-trivial to validate a parameter, which
could have many different correctness properties, such as type,
range, semantic meaning, dependencies with other parameters,
etc.; encoding all of them into validators is laborious and
error-prone, not to mention the maintenance cost [93], [94].
Using machine learning (ML) and natural language process-
ing (NLP) to detect misconfigurations has been considered
a promising approach to addressing the above challenges.
Compared to manually written static validators, ML or NLP-
based approaches are automatic, easy to scale to a large number
of parameters, and applicable to different projects. Several
ML/NLP-based misconfiguration detection techniques were
proposed [14], [35], [56], [57], [72], [77], [83], [92]. The key
idea is to first learn correctness rules from field configuration
data [14], [34], [35], [56], [65], [66], [72], [77], [92] or from
documents [57], [83], and then use the learned rules to detect
misconfigurations in new configuration files. ML/NLP-based
approaches have achieved good success. For example, Microsoft
adopted PeerPressure [30], [72] as a part of Microsoft Product
Support Services (PSS) toolkits. It collects configuration data
in Windows Registry from a large number of Windows users
to learn statistical golden states of system configurations.
However, ML/NLP-based misconfiguration detection is also
significantly limited. First, the need for large volumes of system-
specific configuration data makes it hard to apply those tech-
niques outside corporations that collect user configurations (e.g.,
Windows Registry [77]) or maintain a knowledge base [57]. For
example, in cloud systems where the same set of configurations
is maintained by a small DevOps team [68], [70], there is
often no enough information for learning [86]. Moreover,
prior ML/NLP-based detection techniques all target specific
projects, and rely on predefined features [56], templates [92],
or models [57], making them hard to generalize.
Recent advances on Large Language Models (LLMs), such as
GPT [2] and Codex [3], show promises to address some of the
long-lasting limitations of traditional ML/NLP-based miscon-
figuration detection techniques. Specifically, LLMs are trained
on massive amounts of public data, including configuration
data—configuration files in software repositories, configuration
documents, knowledge-based articles, Q&A websites for resolv-
ing configuration issues, etc. Hence, LLMs encode extensive
arXiv:2310.09690v2  [cs.SE]  2 Apr 2024

knowledge of both common and project-specific configuration.
Such knowledge can be utilized for configuration validation
without the need for manual rule engineering. Furthermore,
LLMs show the capability of generalization and reasoning [28],
[78] and can potentially “understand” configuration semantics.
For example, they can not only understand that values of a
port must be in the range of [0, 65535], but also reason that a
specific configuration value represents a port (e.g., based on
the name and description) and thus has to be within the range.
Certainly, LLMs have limitations. They are known for
hallucination and non-determinism [10], [95]. Additionally,
LLMs have limited input context, which can pose challenges
when encoding extensive contexts like configuration file and
related code. Moreover, they are reported to be biased to popular
content in the training dataset. Fortunately, active efforts [8],
[42], [44], [51], [76] are made to address these limitations.
In this paper, we present a first analysis on the feasibility
and effectiveness of using LLMs such as GPT and Claude for
configuration validation. As a first step, we empirically evaluate
LLMs in the role of configuration validators, without additional
fine-tuning or code generation. We focus on basic misconfigu-
rations (those violating explicit correctness constraints) which
are common misconfigurations encountered in the field [90].
We do not target environment-specific misconfigurations or
bugs triggered by configuration (§VII).
To do so, we develop Ciri, an LLM-empowered configuration
validation framework. Ciri takes a configuration file or a file diff
as the input; it outputs detected misconfigurations along with
the reasons that explain them. Ciri integrates different LLMs
such as GPT-4, Claude-3, and CodeLlama. Ciri devises effective
prompt engineering with few-shot learning based on existing
configuration data. Ciri also validates the outputs of LLMs to
generate validation results, coping with the hallucination and
non-determinism of LLMs. A key design principle of Ciri is
separation of policy and mechanism. Ciri can serve as an open
framework for experimenting with different models, prompt
engineering, training datasets, and validation methods.
We study Ciri’s validation effectiveness using eight popular
LLMs including remote models (GPT-4, GPT-3.5, Claude-
3-Opus, and Claude-3-Sonnet), and locally housed models
(CodeLlama-7B/13B/34B and DeepSeek). We evaluate ten
widely deployed open-source systems with diverse types. Our
study confirms the potential of using LLMs for configuration
validation, e.g., Ciri with Claude-3-Opus detects 45 out of 51
real-world misconfigurations, outperforming recent configura-
tion validation techniques. Our study also helps understand the
design space of LLM-based validators like Ciri, especially in
terms of prompt engineering with few-shot learning and voting.
We find that using configuration data as shots can enhance
validation effectiveness. Specifically, few-shot learning using
both valid configuration and misconfiguration data achieves the
highest effectiveness. Our results also reveal open challenges:
Ciri struggles with certain types of misconfigurations such as
dependency violations and version-specific misconfigurations.
It is also biased to the popularity of configuration parameters,
causing both false positives and false negatives.
In summary, this paper makes the following contributions:
• A new direction of configuration validation using pre-trained
large language models (LLMs);
• Ciri, an LLM-empowered configuration validation framework
and an open platform for configuration research;
• An empirical analysis on the effectiveness of LLM-based
configuration validation, and its design space;
• We have released Ciri and other research artifacts at https:
//github.com/ciri4conf/ciri.
II. EXPLORATORY EXAMPLES
We explore using LLMs to validate configuration out of the
box. We show that vanilla LLMs can detect misconfigurations.
However, they are prone to both false negatives and false
positives that require careful handling. Figure 1 presents
four examples, two of which the LLM successfully detects
misconfigurations, and two of which the LLM misses the
misconfiguration or reports a false alarm. These examples were
generated using the GPT-3.5-Turbo LLM [97].
Detecting violation of configuration dependency. Validating
dependencies between configuration parameters has been a
challenging task in highly-configurable systems [18], [89].
LLMs can infer relations between entities from text at the
level of human experts [15], which allows LLMs to infer
dependencies between parameters in a given configuration file
based on their names and descriptions. Figure 1 (Example 1)
presents a case where values of two dependent parameters
were changed (i.e., “buffer.size” and “bytes.per.checksum)”.
After understanding the value relationship dependency between
these two parameters, the model determines that the change
in “bytes.per.checksum” has violated the enforced dependency,
and provides the correct reason for the misconfiguration.
Detecting violation with domain knowledge. A state-of-the-
art LLM is trained on a massive amount of textual data and
possesses basic knowledge across a wide range of professional
domains. An LLM thus could be capable of understanding the
definition of a configuration parameter and reasoning with its
semantics. When the LLM encounters a configuration parameter
such as IP address, permissions, and masks, it invokes the
domain knowledge specific to the properties of those parameters.
Figure 1 (Example 2) presents a case where an HTTP address
has been misconfigured to a semantically invalid value. The
model detects the misconfiguration, reasons that its value is
out of range, and further suggests a potential fix.
Missed misconfiguration and false alarm. LLMs as config-
uration validators are not without errors. Examples 3 and 4 in
Figure 1 show two cases where the LLM makes mistakes.
In Example 3, the configuration file has provided a descrip-
tion of the changed parameter “hostname.verifier” and explicitly
listed the valid value options of the parameter. However, the
model fails to realize that the parameter is misconfigured to
an invalid, non-existent option (STRICT I8). In Example 4,
the description suggests that the parameter “bloom.error.rate”
ranges from 0 to 100 (percentage), whereas the actual scale is
2

<name>io.file.buffer.size</name>
<value>4096</value>
<description>The size of buffer for use   
in sequence files during read and  
write operations.</description>
<name>io.bytes.per.checksum</name>
<value>4097</value>
<description>The number of bytes per 
checksum. Must not be larger than 
io.file.buffer.size.</description> ...
Q: Any mistakes in the above configuration file?
The value of io.bytes.per.checksum should be 
less than or equal to io.file.buffer.size.
Config.
A:
<name>dfs.journalnode.httpaddress
</name>
<value>0.0.0.0:80800</value>
<description>
The address and port the
JournalNode HTTP server listens
on.
</description> 
...
The port number is too big. The port number 
should be less than 65535.
Example 1: LLM catches a dependency violation
<name>hadoop.ssl.hostname.verifier
</name>
<value>STRICT_I8</value>
<description>
The hostname verifier to provide
for HttpsURLConnections. Valid
values are DEFAULT,STRICT,
STRICT_I6, DEFAULT_AND_LOCALHOST
and ALLOW_ALL
</description> 
...
<name>io.mapfile.bloom.error.rate</name>
<value>0.01</value>
<description>
The rate of false positives in
BloomFilter-s used in BloomMapFile.
As this value decreases, the size
of BloomFilter-s increases
exponentially. This value is the
probability of encountering false
positives (default is 0.5%). 
</description> ...
Example 3: LLM misses an invalid option
Example 4: LLM reports a false alarm
The value of the property hadoop.security.-
crypto.cipher.suite is wrong.
The value of the property io.mapfile.bloom.-
error.rate is 0.01, which is too low.
Example 2: LLM catches an invalid port value
Any mistakes in the above configuration file?
Any mistakes in the above configuration file? Any mistakes in the above configuration file?
Fig. 1: Example 1 and 2 show the LLM correctly catches and reasons the misconfigurations. Example 3 and 4 show the LLM
misses the misconfiguration or reports a valid configuration as erroneous.
0 to 1 (fraction). This inconsistency supposedly confuses the
model making it mark 0.01 (a valid value) as invalid.
Both examples show that directly using off-the-shelf LLMs
as configuration validators would result in false negatives and
false positives. The incorrect validation results can be attributed
to hallucination [54]. A simple explanation is that LLMs are
exposed to potentially contradictory data during training, which
causes confusion to the model at the inference time.
III. CIRI: A LLM-EMPOWERED CONFIGURATION
VALIDATION FRAMEWORK
We develop Ciri, an LLM-empowered configuration valida-
tion framework. Ciri takes a configuration file or a file diff
as the input, and outputs a list of detected misconfigurations
along with the reasons to explain the misconfigurations. Ciri
supports different LLMs such as GPT, Claude, CodeLlama,
and DeepSeek [54], [64].1
Figure 2 gives an overview of Ciri. Ciri turns a configuration
validation request into a prompt to the LLMs (§III-A). The
prompt includes (1) the target configuration file or diff, (2) a few
examples (aka shots) to demonstrate the task of configuration
validation, (3) code snippets automatically extracted from
codebase, and (4) directive question and metadata. To generate
shots, Ciri uses its database that contains labeled configuration
data, including both valid configurations and misconfigurations.
Ciri sends the same query to the LLMs multiple times and
aggregates responses into the final validation result (§III-B).
Ciri applies to any software project, even if it has no labeled
configuration data of that project in its database. Ciri exhibits
transferability (using data from one project and applying it to
others), the ability to transfer configuration-related knowledge
across projects when using configurations from different
projects as shots (Finding 4). Ciri’s configuration validation
effectiveness can also be further improved by generating quality
shots (Finding 3) and code snippets (Finding 5).
A. Prompt Engineering
1) Prompt structure: Ciri generates a prompt that includes
four elements: a) the content of input configuration file or file
diff, b) the shots as valid configurations or misconfigurations
1Adding a new LLM in Ciri takes a few lines to add the query APIs.
Ciri 
Config.
Ciri
Query
LLM
Shot File Selection
Labeled Misconfiguration shot
Valid configuration shot
Answer...
Are there any mistakes...
<name>io.bytes.per.checksum</name>
<value>4097</value>
Q:
Configuration Shot File
Config.
A:
Prompt Generation
Config.
shots
Config.
to validate
Result Generation
•
Validation
•
Voting
Response
Configuration 
Database
User
Result
Automatic Code Retrieval
Code 
Cache
Code 
Retrieval
Code Snippet
Fig. 2: System overview of Ciri.
with questions and ground truth responses for few-shot learn-
ing, c) code snippets automatically extracted from available
codebase, and d) a directive question for LLM to respond in
formatted output. Figure 3 shows an illustrative example of the
prompt generated by Ciri. It contains N shots, the content of
to-be-validated configuration file, and the code snippet enclosed
within ⟨Usage⟩followed by the directive question.
Ciri phrases the prompting question as “Are there any
mistakes in the above configuration for [PROJECT] version
[VERSION]? Respond in a JSON format similar to the
following: ...”. The [PROJECT] and [VERSION] are required
inputs of Ciri because the validity of configuration can change
by project and project version [93], [94]. This prompt format
enforces the LLM to respond in a unified JSON format for
result aggregation (§III-B). However, responses from LLMs
sometimes may still deviate from the anticipated format [10],
[95]. In such cases, Ciri retries a new query to the LLM.
2) Few-shot learning: Ciri leverages the LLM’s ability to
learn from examples at inference time (aka few-shot learning)
to improve configuration validation effectiveness. To do so, Ciri
simply inserts shots at the beginning of each prompt. Each shot
contains a configuration snippet, the prompting question, and its
corresponding ground truth. Figure 3 shows an example, where
there are N shots. “Configuration File Shot #1” is the first shot,
in which the parameter “yarn.resourcemanager.hostname” is
misconfigured. This shot also contains the prompting question
(orange box) and the ground truth (blue box).
3) Shot generation: Ciri maintains a database of labeled
valid configurations and misconfigurations for generating valid
configuration shots (ValidConfig) and misconfiguration shots
3

<name>yarn.resourcemanager.hostname</name>
<value>192.168.256.1</value>
<description>Address of applications manager interface in the RM.
</description> ...
Question: Any mistakes in the above configuration file for YARN 
version 3.3.0? Respond in a json format similar to the following:
{
"hasError": boolean, // true if there are errors, false if none
"errParameter": [], // List containing properties with errors
"reason": [], // List containing explanations for each error
}
Configuration File Shot #1
Answer:
{
"hasError": true,
"errParameter": ["yarn.resourcemanager.hostname"],
"reason": ["Each octet (segment of the IP address, separated by  
dots) must be in the range of 0 to 255."]
}
<name>yarn.webapp.address</name>
<value>nm.company.com:8042</value>
<description>NM Webapp address.</description>
<Usage>port=conf.get("yarn.webapp.address").split(":")[1];</Usage>
Question: ...
……
Configuration File Shot #N
To Be Validated Configuration File
Fig. 3: An example prompt generated by Ciri.
(Misconfig). A ValidConfig shot specifies a set of configuration
parameters and their valid values. A valid value of a parameter
can be its default value, or other valid values used in practice.
A Misconfig shot specifies a set of parameters and their values,
where only one of the parameter values is invalid.
For a given configuration of a specific project, Ciri by default
generates shots using configuration data of the same project.
If Ciri’s database does not contain configuration data for the
target project, Ciri will use data from other projects to generate
shots. As shown in Finding 4, LLMs possess transferrable
knowledge in configuration across different projects.
Ciri supports multiple methods for selecting data to generate
shots, including randomized selection, category-based selection,
and similarity-based selection (selecting data from configuration
with the highest cosine similarity). We did not observe major
differences when using different selection methods. So, Ciri
uses randomized selection by default.
4) Augmenting with code: Recent work shows that retrieval-
augmented generation (RAG) can enhance LLMs by incor-
porating additional information [27], [37]. In the context of
configuration, each configuration parameter has corresponding
program context, such as data type, semantics, and usage.
In Figure 3, the code snippet enclosed within ⟨Usage⟩is
automatically extracted from the source code, which uncovers
semantics that the parameter value is expected to include a “:”
symbol, with the split segment representing a port.
Ciri uses a simple but effective code retrieval strategy: To
retrieve the most effective code snippet, Ciri employs the
following strategy: a) searching codebase with parameter names
and retrieving relevant snippets; b) prioritizing code over
comments and documents; c) selecting the longest snippet
if multiple options are available, as longer snippets tend to be
more comprehensive with details, and d) deduplicating retrieved
snippets. The retrieved code snippet will be stored in a cache for
efficiency. The code retrieval strategy is effective in improving
the effectiveness of configuration validation (Finding 5).
5) Addressing token limits: LLMs limit input size per query
by the number of input tokens. For example, the token limits for
GPT-3.5-Turbo are 16,385. To navigate these constraints, Ciri
compresses the prompt if its size exceeds the limit. Ciri first
tries to put the target configuration and the directive question
in the prompt, then maximizes three Misconfig shots with one
ValidConfig shot (Finding 3) to fit into the remaining space. If
the configuration cannot fit into the token limit, Ciri transforms
it into a more compact format, e.g., transforming an XML file
into INI format. If the compressed input still cannot fit, Ciri
aborts and returns errors. In practice, configuration files and
diffs are small [70], [85] and can easily fit existing limits. For
example, prior study inspects configuration files collected from
Docker, where each file contains 1 to 18 parameters, with eight
on average [1]. For very large configurations, Ciri can split
them into multiple snippets and validate them separately.
B. Result Generation
The JSON response from LLMs contains three primary fields:
a) “hasError”: a boolean value indicating whether misconfigu-
rations are detected, b) “errParameter”: a list of misconfigured
parameters, and c) “reason”: a list of explanations of the
detected misconfiguration, corresponding to “errParameter”.
1) Validation against hallucination: We employ a few rules
to address the hallucination of LLMs. For example, if hasError
is false, both errParameter and reason must be empty. Similarly,
if hasError returns true, errParameter and reason must be non-
empty with the same size. The answer to “errParameter” must
not contain repeated values. If a response fails these rules, Ciri
retries until the LLM returns a valid response.
2) Voting against inconsistency: LLMs can produce in-
consistent outputs in conversation [5], explanation [16], and
knowledge extraction [22]. To mitigate inconsistency, Ciri uses
a multi-query strategy—querying the LLM multiple times using
the same prompt and aggregating responses towards a result
that is both representative of the model’s understanding and
more consistent than a single query. Ciri uses a frequency-
based voting strategy: the output that recurs most often among
the responses is selected as the final output [76].
Note that the “reason” field is not considered during voting
due to the diverse nature of the response. After voting, Ciri
collects reasons from all responses associated with the selected
errParameter. The reason field is important as it provides users
with insights into the misconfiguration, which is different from
the traditional ML approaches that only provide a binary answer
with a confidence score. Ciri clusters the reasons based on
TF-IDF similarity [21], and picks a reason from the dominant
cluster. We find that this mechanism is robust to hallucination—
hallucinated reasons were often filtered out as they tended to
be very different from each other.
C. Ciri Configuration
Ciri is customizable, with a key principle of separating
policy and mechanism. Users can customize Ciri via its
4

own configurations. Table I shows several important Ciri
configurations and default values. The default values are chosen
by pilot studies using a subset of our dataset (§IV).
IV. BENCHMARKS AND METRICS
Our study evaluates ten mature and widely deployed open-
source projects: Alluxio, Django, Etcd, HBase, Hadoop Com-
mon, HDFS, PostgreSQL, Redis, YARN, ZooKeeper, which
are implemented in a variety of programming languages (Java,
Python, Go, and C). They also use different configuration
formats (XML and INI) with a large number of configuration
parameters. Table III lists the version (SHA) and the number
of parameters at that version.
We evaluate Ciri on the aforementioned projects with eight
LLMs: GPT-4-Turbo, GPT-3.5-Turbo, Claude-3-Opus, Claude-
3-Sonnet, CodeLlama-7B/13B/34B, and DeepSeek-6.7B, which
differ in model sizes and capabilities. All of these models have
also been trained with a large amount of code data, where prior
work has demonstrated their promising capability in handling
a number of software engineering tasks [19], [71], [82].
A. Configuration Dataset
Our study uses two types of datasets: real-world misconfig-
uration datasets and synthesized misconfiguration datasets.
1) Real-world misconfiguration: To our knowledge, the
Ctest dataset [1] is the only public dataset of real-world miscon-
figurations; it is used by prior configuration research [14], [20],
[69], [73]. The dataset contains 64 real-world configuration-
induced failures of five open-source projects, among which 51
are misconfigurations, and 13 are bugs. We discuss the results
of Ciri on real-world misconfigurations in Finding 2.
2) Synthesized misconfiguration: Since real-world configu-
ration dataset (§IV-A1) is too small, to systematically evaluate
configuration validation effectiveness, we create new synthetic
datasets for each evaluated project. First, we collect default
configuration values from the default configuration file of
each project, and real-world configuration files from the Ctest
dataset (collected from Docker images [1]) for those projects
included in Ctest. We then generate misconfigurations of
different types. The generation rules are from prior studies
on misconfigurations [32], [38], [39], [88], which violates the
constraints of configuration parameters (Table II). Notably,
prior studies show that the generation rules can cover 96.5%
of 1,582 parameters across four projects [38].
For each project, we build two distinct configuration sets.
First, we build a configuration dataset with no misconfiguration
(denoted as ValidConfig) to measure true negatives and false
positives (Table IV). We also build a configuration dataset
(denoted as Misconfig) in which each configuration file has one
misconfiguration, to measure true positives and false negatives
TABLE I: configuration of Ciri and its default values.
Parameter
Description
Default Value
Model
Backend LLM. Also allows users to add other LLMs.
GPT-4
Temperature Tradeoff between creativity and determinism.
0.2
# Shots
The number of shots included in a prompt.
Dynamic
# Queries
The number of queries with the same prompt.
3
(Table IV). Note that a misconfiguration can be a dependency
violation between multiple parameter values.
To create the Misconfig data for each project, we first check
if its configuration parameters fit any subcategory in Table II,
and, if so, we apply rules from all matched subcategories to
generate misconfigurations for that parameter. For example,
an IP-address parameter fits both “Syntax: IP Address” and
“Range: IP Address”. We do so for all parameters in the
project. Then, we randomly sample at most five parameters in
each subcategory that has matched parameters, and generate
invalid value(s) per sampled parameter. For each subcategory,
we further randomly select one parameter from the five
sampled ones. We use the selected parameter to create a faulty
configuration as a Misconfig shot (§III) for that subcategory and
add it to the project’s shot pool. For the other four parameters,
we use them to create four faulty configurations for that
subcategory, and use them for evaluation. If a subcategory
does not have enough parameters for sampling, we use all the
parameters for evaluation. We separate the evaluation set and
shot pool to follow the practice that the training set does not
overlap with the testing set [15]. We create the ValidConfig
dataset for each project using the aforementioned methodology
for the Misconfig dataset, except that we generate valid values.
Table III shows the size for both the ValidConfig and
Misconfig datasets for each project. Note that our datasets
cover 72%–100% of the entire parameter set of each project.
B. Metrics
We evaluate Ciri’s effectiveness at both configuration file
and parameter levels: (1) at the file level, we check if Ciri can
determine if a configuration file contains misconfigurations; (2)
at the parameter level, we check if Ciri can determine if each
parameter in the configuration file is valid or not. Table IV
describes our confusion matrix. We compute the precision
(TP/(TP+FP)), recall (TP/(TP+FN)), and F1-score at both file
and parameter levels. If not specified, we default to macro
averaging since each project is regarded equally. We prioritize
parameter-level effectiveness for fine-grained measurements
and discuss parameter-level metrics by default in the evaluation.
V. EVALUATION AND FINDINGS
We present empirical results on the effectiveness of LLMs
as configuration validators with Ciri (§V-A). We analyze how
validation effectiveness changes with regard to design choices
of Ciri (§V-B). We also present our understanding of when
Ciri produces wrongful results (§V-C) and biases (§V-D).
A. Effectiveness of Configuration Validation
Finding 1. Ciri shows effectiveness of using state-of-the-
art LLMs as configuration validators. It achieves file- and
parameter-level F1-scores up to 0.79 and 0.65, respectively.
Ciri exhibits remarkable capability in configuration validation.
Table V shows the F1-score, precision, and recall for each
project using LLMs with three Misconfig and one ValidConfig
shots (Finding 3). The results show that Ciri not only can
effectively identify configuration files with misconfiguration
5

TABLE II: Misconfiguration generation (we use generation rules from prior work [32], [38], [39], [69], which reflects real-world
misconfiguraions). “Subcategory” lists rules to generate different misconfiguraions for the same configuration parameter.
Category
Subcategory
Specification
Generation Rules
Syntax
Value set = {Integer, Float, Long...}
Generate a value that does not belong to the value set
Data type
Numbers with units
Generate an invalid unit (e.g., ”nounit”)
Path
ˆ(\/[ˆ\/ ]*)+\/?$
Generate a value that violates the pattern (e.g., /hello//world)
URL
[a-z]+://.*
Generate a value that violates the pattern (e.g., file///)
IP address
[\d]{1,3}(.[\d]{1,3}){3}
Generate a value that violates the pattern (e.g., 127.x0.0.1)
Port
Data type, value set = {Octet}
Generate a value that does not belong to the value set
Permission
Data type, value set = {Octet}
Generate a value that does not belong to the value set
Range
Basic numeric
Valid Range constrainted by data type
Generate values outside the valid range (e.g., Integer.MAX VALUE+1)
Bool
Options, value set = {true, false}
Generate a value that does not belong to the value set
Enum
Options, value set = {“enum1”, “enum2”, ...}
Generate a value that doesn’t belong to set
IP address
Range for each octet = [0, 255]
Generate a value outside the valid range (e.g., 256.123.45.6)
Port
Range = [0, 65535]
Generate a value outside the valid range
Permission
Range = [000, 777]
Generate a value outside the valid range
Dependency Control
(P1, V, ♢) 7→P2, ♢∈{>, ≥, =, ̸=, <, ≤}
Generate invalid control condition (P1, V, ¬♢)
Value Relationship (P1, P2, ♢), ♢∈{>, ≥, =, ̸=, <, ≤}
Generate invalid value relationship (P1, P2, ¬♢)
Version
Parameter change
(V1, Pset1) 7→(V2, Pset2), Pset1 ̸= Pset2 Generate a removed parameter in V2 or use an added paraemter in V1
TABLE III: Evaluated projects and the configuration datasets
(ValidConfig and Misconfig) for shot pool and evaluation.
Project
Version
# Params
ValidConfig
Misconfig
(SHA)
# Shot # Eval
# Shot # Eval
Alluxio
76569bc
494
13
54
13
54
Django
67d0c46
140
6
18
6
18
Etcd
946a5a6
41
8
32
8
32
HBase
0fc18a9
221
12
50
12
50
HCommon
aa96f18
395
16
64
16
64
HDFS
aa96f18
566
16
64
16
64
PostgreSQL 29be998
315
8
31
8
31
Redis
d375595
94
12
44
12
44
YARN
aa96f18
525
10
40
10
40
ZooKeeper
e3704b3
32
8
32
8
32
(with an average F1-score of 0.72 across 8 LLMs), but also
pinpoint misconfigured parameters with explanations (with
an average F1-score of 0.56 across 8 LLMs). Certainly, the
parameter-level F1-scores are about 15% lower than file-
level F1-scores, i.e., pinpointing fine-grained misconfigured
parameters is a more challenging task for LLMs compared to
classifying the entire file as a whole.
Finding 2. Ciri detects 45 out of 51 real-world misconfigura-
tions, outperforming recent configuration validation techniques,
including learning-based [14] and configuration testing [69].
We conduct experiments to evaluate how Ciri compares with
existing validation techniques on a real-world dataset. For this
evaluation, we choose the top five LLMs ranked by F1-score
at the parameter-level based on the results from Table V. The
real-world dataset [1] contains 51 misconfigurations in total
(§IV), among which Ciri can detect 33-45 misconfigurations,
as shown in Table VI. Ciri successfully detected 45 using
Claude-3-Opus. The six undetected misconfigurations include
three due to parameter dependency violations (discussed further
in §V-C), and the other three are environment-related issues
that are beyond Ciri’s current capability. Notably, Ciri seldom
reports incorrect detection.
We compare Ciri’s results with a recent learning-based
validation technique, ConfMiner [14], which was evaluated
on the same dataset. ConfMiner utilizes the file content
and commit history to identify patterns in configuration to
detect misconfigurations. ConfMiner can detect 27 out of 51
misconfigurations, which is 40% less than Ciri. Unlike LLMs
that are trained on extensive text data and can comprehend
the context of configurations, ConfMiner relies on regular
expressions to identify patterns. This approach limits its ability
in complex scenarios, such as identifying valid values for
enumeration parameters and understanding the relationships
between different parameters.
We also compare Ciri with a recent configuration testing
technique, namely Ctest [20], [69], [73]. Ctest detected 41 of
the real-world misconfigurations without rewriting test code;
Ciri outperforms Ctest by 8.9%. The reasons are twofold. First,
testing relies on adequacy of the test cases. We find that existing
test suites do not always have a high coverage of configuration
parameters. On the other hand, LLMs can validate any param-
eter. Second, LLMs detected “silent misconfigurations” [90]
that are not manifested via crashes or captured by assertions
(e.g., several injected misconfigurations silently fell back to
default values and passed the test; LLMs detected them likely
because they violated documented specifications).
Certainly, Ctest can detect a broader range of misconfigura-
tions such as the environment-related issues that Ciri cannot.
We do not intend to replace configuration testing with LLMs.
Instead, our work shows that LLMs can provide much quicker
feedback for common types of misconfigurations, so tools
like Ciri can be used in an early phase (e.g., configuration
authoring) before running expensive configuration testing. As
shown in Table VI, for a configuration file in the real-world
dataset, Ctest takes 20 to 230 minutes to finish [69], while Ciri
only takes 10 to 70 seconds.
In summary, our results show that LLMs like GPT, Claude-3,
and CodeLlama-34B can effectively validate configurations and
detect misconfigurations with a sensibly designed framework
like Ciri. Ciri can provide prompt feedback, complementing
other techniques like configuration testing.
6

TABLE V: F1-score, precision, and recall of Ciri evaluated on ten projects with eight LLMs as configuration validators.
Models
F1-score
Precision
Recall
File-Level (F.L.)
Parameter-Level (P.L.)
F.L. P.L.
F.L. P.L.
AL.
DJ.
ET.
HB. HC. HD. PO.
RD. YA.
ZK.
Avg
AL.
DJ.
ET.
HB. HC. HD. PO.
RD. YA.
ZK.
Avg
Avg Avg
Avg Avg
GPT-4-Turbo
0.69 0.86 0.67 0.73 0.75 0.70 0.72 0.75 0.74 0.70 0.73
0.52 0.82 0.59 0.49 0.51 0.53 0.43 0.57 0.62 0.53 0.56
0.62 0.44
0.89 0.81
GPT-3.5-Turbo
0.68 0.71 0.73 0.77 0.78 0.68 0.65 0.71 0.72 0.74 0.72
0.48 0.55 0.60 0.55 0.58 0.55 0.36 0.54 0.61 0.66 0.55
0.62 0.43
0.89 0.77
Claude-3-Opus
0.71 0.81 0.70 0.70 0.79 0.74 0.75 0.82 0.77 0.78 0.76
0.51 0.62 0.53 0.54 0.65 0.60 0.53 0.62 0.60 0.60 0.58
0.65 0.45
0.91 0.83
Claude-3-Sonnet 0.74 0.77 0.79 0.80 0.81 0.76 0.82 0.84 0.79 0.78 0.79
0.53 0.65 0.69 0.73 0.69 0.73 0.53 0.64 0.71 0.59 0.65
0.75 0.57
0.85 0.79
CodeLlama-34B
0.69 0.87 0.80 0.64 0.70 0.67 0.79 0.78 0.66 0.83 0.74
0.61 0.85 0.76 0.35 0.45 0.35 0.59 0.65 0.46 0.79 0.59
0.65 0.51
0.89 0.70
CodeLlama-13B
0.71 0.67 0.67 0.71 0.66 0.70 0.71 0.76 0.69 0.77 0.70
0.54 0.59 0.61 0.48 0.37 0.37 0.50 0.69 0.51 0.73 0.54
0.61 0.45
0.85 0.68
CodeLlama-7B
0.67 0.80 0.74 0.67 0.67 0.67 0.63 0.73 0.67 0.76 0.70
0.53 0.67 0.66 0.27 0.28 0.23 0.51 0.68 0.43 0.72 0.50
0.56 0.40
0.96 0.67
DeepSeek-6.7B
0.72 0.72 0.81 0.52 0.47 0.46 0.70 0.67 0.59 0.84 0.65
0.58 0.56 0.75 0.48 0.40 0.37 0.44 0.44 0.55 0.73 0.53
0.76 0.60
0.66 0.55
TABLE IV: Definitions for confusion matrix.
Level
Metric Definition
File
TP
A misconfigured file correctly identified
FP
A correct file wrongly flagged as misconfigured
TN
A correct file rightly identified as valid
FN
A misconfigured file overlooked or deemed correct
Param.
TP
A misconfigured parameter correctly identified
FP
A correct parameter wrongly flagged as misconfigured
TN
A correct parameter rightly identified as valid
FN
A misconfigured parameter overlooked or deemed correct
TABLE VI: A comparison of Ciri, ConfMiner, and Ctest in
detecting real-world misconfigurations. N.R.: “not reported.”
Technique
# Correct
# Incorrect # Missed Runtime
Detection
Detection
Ciri (Claude-3-Opus)
45 (88.2%)
1
5
20-60 sec
Ciri (GPT-4-Turbo)
41 (80.4%)
2
8
15-40 sec
Ciri (CodeLlama-34B)
39 (76.5%)
1
11
30-70 sec
Ciri (GPT-3-Turbo)
37 (72.5%)
3
11
10-25 sec
Ciri (Claude-3-Sonnet) 33 (67.7%)
1
17
10-30 sec
ConfMiner
27 (52.3%)
N.R.
N.R.
N.R.
Ctest
41 (80.4%)
N.R.
N.R.
20-230 min
B. Impacts of Design Choices
Ciri plays a critical role in LLMs’ effectiveness of configu-
ration validation. We explore its design choices and impacts.
Finding 3. Using configuration data as shots can effectively
improve LLMs’ effectiveness of configuration validation. Shots
including both valid configuration and misconfiguration achieve
the highest effectiveness.
Using validation examples as shots can effectively improve the
effectiveness of LLMs. Table VII shows the results of LLMs
when the validation query does not include shots. In particular,
comparing Table VII to Table V, as indicated by the numbered
arrows in Table VII, the average F1-score of the LLMs has
decreased by 0.03–0.54 at the file level, and decreased by
0.21–0.47 at the parameter level.
We also study Ciri’s effectiveness with different shot com-
binations. We evaluate six N-shot learning settings, where
N ranges from 0 to 5. For example, to evaluate Ciri with a
two-shot setting, three experiments will be performed: (1) two
ValidConfig shots; (2) one ValidConfig shot plus one Misconfig
shot; (3) two Misconfig shots. In total, we experiment with
21 shot combinations. Due to cost, we only run experiments
on GPT-3.5-Turbo to HCommon. We find that only using
ValidConfig shots leads to a decrease in precision, while only
TABLE VII: Effectiveness of LLMs without using shots.
Models
F1-score
Precision
Recall
F.L.
P.L.
F.L.
P.L.
F.L.
P.L.
GPT-4-Turbo
0.70 (0.03↓)
0.34 (0.22↓)
0.57
0.23
0.93
0.82
GPT-3.5-Turbo
0.67 (0.05↓)
0.20 (0.35↓)
0.50
0.12
0.99
0.77
Claude-3-Opus
0.69 (0.07↓)
0.37 (0.21↓)
0.64
0.28
0.82
0.69
Claude-3-Sonnet
0.67 (0.12↓)
0.28 (0.37↓)
0.55
0.20
0.89
0.66
CodeLlama-34B
0.66 (0.08↓)
0.12 (0.47↓)
0.50
0.07
0.96
0.52
CodeLlama-13B
0.59 (0.11↓)
0.12 (0.42↓)
0.53
0.07
0.76
0.52
CodeLlama-7B
0.65 (0.05↓)
0.11 (0.39↓)
0.51
0.08
0.91
0.23
DeepSeek-6.7B
0.11 (0.54↓)
0.06 (0.47↓)
0.99
0.50
0.06
0.04
0
1
2
3
4
5
0
1
2
3
4
5
0.67
0.67
0.62
0.65
0.64
0.66
0.68
0.66
0.71
0.78
0.70
0.64
0.67
0.68
0.72
0.69
0.67
0.68
0.72
0.69
0.65
File-Level
0
1
2
3
4
5
0
1
2
3
4
5
0.20
0.44
0.52
0.54
0.56
0.57
0.29
0.43
0.49
0.58
0.57
0.34
0.36
0.48
0.49
0.40
0.42
0.46
0.44
0.46
0.39
Param-Level
# ValidConfig Shot
# Misconfig Shot
Fig. 4: F1 scores under different shot combinations.
using Misconfig shots reduces recall. Clearly, text distribution in
the query affects LLMs [48]. LLMs can be biased: if the shots
are all misconfigurations, LLMs will be overly sensitive to the
specific patterns in the shots, known as overfitting, which causes
LLMs miss other types of misconfigurations; if the shots are
all ValidConfig, LLMs face challenges in accurately identifying
incorrect parameters within the file, leading to false alarms. As
shown in Figure 4, using both Misconfig and ValidConfig in
few-shot learning mitigates the biases and achieves the highest
effectiveness, and including three Misconfig shots and one
ValidConfig shot in the prompt achieves the highest F1-score
at both the file and parameter levels.
Finding 4. Using configuration data from the same project as
shots often leads to high validation F1 score. However, even
without access to configuration data from the target project,
using configuration data from a different project can lead to a
improved validation score than zero-shot.
In situations where configuration data is unavailable (e.g.,
due to confidentiality), we evaluate whether using configuration
data from other systems as shots can improve configuration
validation effectiveness on the target system. Table VIII shows
7

TABLE VIII: F-1 score on HCommon (HC.) using shots from
different systems, e.g., HB. refers to using HBase shots. 4-S
and 0-S means using four shots and no shots respectively.
Models
File-Level (F.L.)
Parameter-Level (P.L.)
HC.
Dj. ET. HB. Avg
HC.
Dj. ET. HB. Avg
4-S 0-S
4-S 0-S
GPT-3.5-Turbo
0.780.67 0.780.680.74 0.74 0.580.20 0.440.420.51 0.46
Claude-3-Sonnet 0.810.67 0.740.740.78 0.75 0.690.28 0.540.590.67 0.60
the results of using data from other projects as shots for
configuration validation on HCommon. By comparing 4-shot
HCommon with other columns in Table VIII, we see that using
shots from other projects is not as effective as using shots from
the target system. However, the average F1-score is still higher
than zero-shot, indicating that using shots from other projects
can improve the effectiveness over zero-shot. Our observations
highlight that Ciri with LLMs can transfer configuration-related
knowledge across different projects for effective configuration
validation compared to traditional approaches.
Finding 5. Ciri’s code augmentation approach can help LLMs
to better understand the context of the configuration and
improve the validation effectiveness.
We compare the results of GPT-3.5-Turbo with and without
code augmentation with four shots. The results show an
improvement in F1 scores by 0.03 at both the file and parameter
levels. Figure 5 exemplifies code snippets retrieved from the
codebase by Ciri, which could improve LLMs’ comprehension
of the configuration context: (1) examples 1 and 2 delineate
the parameter types as Integer and Boolean, respectively. (2)
example 3 highlights that the parameter should include a “:”
symbol, with the latter segment representing a port. (3) example
4 shows that “kerberos” is one valid value for the parameter.
conf.setInt("tfile.fs.input.buffer.size", fsInputBufferSize);
Ex1:
conf.setBoolean("fs.automatic.close", false);
port=conf.get("yarn.nodemanager.webapp.address").split(":")[1];
"kerberos".equals(conf.get("hbase.security.authentication"));
Ex2:
Ex3:
Ex4:
Fig. 5: Code snippets retrieved by Ciri to aid LLMs.
Finding 6. Code-specialized LLMs, e.g., CodeLlama, exhibit
much higher validation scores than generic LLMs, e.g., Llama-
2. Moreover, further scaling up the code-specialized LLMs
leads to a continuous increase in validation scores.
In Table V, we observe a notable trend within the CodeLlama
model family: the 13B model demonstrates an improvement
in F1-score at the parameter level, by 0.04 over the 7B
model; this trend continues with the 34B model, which
exhibits a further 0.05 enhancement in F1-score over the
13B model. The observed performance gains can be primarily
attributed to the increased capacity for learning and representing
complex semantics of configuration values as model size scales.
This involves deep comprehension beyond syntax and range
violations which are more common in practice [90].
We further evaluated the effectiveness of the Llama-2 model,
which is identical to CodeLlama in structure but lacks code-
specific training. The Llama-2-13B is not effective, with an
average F1-score of 0.05 at the parameter level. This result
underscores the role of code-specific training, which enhances
LLM’s comprehension of configuration in the context of code.
C. Limitations and Challenges
Finding 7. With Ciri, LLMs excel at detecting misconfigu-
rations of syntax and range violations with an average F1-
score of 0.8 across subcategories. However, LLMs are limited
in detecting misconfigurations of dependency and version
violations with an average F1-score of 0.3 across subcategories.
Table IX shows Ciri’s validation effectiveness per misconfigura-
tion type. The F1-score on detecting misconfigurations of syntax
and range violations is consistently above 0.5 across projects,
and often reaches 0.8. However, F1-score rarely exceeds 0.5 on
misconfigurations of dependency and version violations. Under
these two categories, LLMs achieve F1-scores of 0.44–0.54
for misconfigurations that violate value relationship, which is
higher than the other two subcategories (control and parameter
change); however, it is still much lower than others.
The difference can be attributed to the inherent nature
of different types of misconfigurations. Misconfigurations of
syntax and range violations are more common in practice [90],
from which LLMs learned extensive knowledge. In such a case,
domain-specific knowledge from LLMs is sufficient to detect
these misconfigurations. But, misconfiguration of dependency
and version violations is often project-specific, as exemplified
in Figure 6. They are tied to detailed history and features of the
project, and thus hard to be captured or memorized by LLMs
if the LLM is not fine-tuned on project-specific data. This
discrepancy between misconfiguration types exposes existing
LLM’s limitation.
<name>hbase.security.authentication</name>
<value>simple</value>
<description>Controls whether secure authentication is enabled 
for HBase. Possible values are ‘simple’ (no authentication), and 
‘kerberos’.</description>
...
<name>hbase.auth.key.update.interval</name>
<value>43200000</value>
<description>The update interval for authentication tokens in 
milliseconds. Used when HBase security is enabled.</description> 
...
Fig. 6: Misconfiguration that violates control dependency that
LLMs cannot detect. The update interval for authentication is
set but the secure authentication is disabled.
Finding 8. Among the detected misconfigurations, LLMs
correctly explained reasons for 93.9% of the misconfigurations;
meanwhile, 6.1% of the reasons are misleading.
When an LLM detects a misconfiguration, Ciri also asks the
LLM to explain the reason to aid debugging and fixing the
misconfiguration (§III-A). To evaluate these explanations, we
randomly select one answer in which the misconfiguration is
correctly detected per ⟨subcategory, project, LLM⟩tuple, and
collect a total of 740 answers (resulting from 2,220 queries).
Upon careful manual review, we determined that 93.9% of
8

TABLE IX: Parameter-level F1-score by misconfiguration types from Table II. N.A. means no evaluation samples.
Category
Sub-category
GPT-4-Turbo
Claude-3-Opus
CodeLlama-34B
AL. DJ.
ET. HB. HC. HD. PO. RD. YA. ZK. Avg AL. DJ.
ET. HB. HC. HD. PO. RD. YA. ZK. Avg AL. DJ.
ET. HB. HC. HD. PO. RD. YA. ZK. Avg
Syntax
Data Type
0.84 1.00 1.00 0.67 0.94 0.89 0.70 0.78 0.89 0.80 0.85 0.67 0.80 1.00 0.80 0.94 1.00 0.80 0.74 1.00 0.80 0.85 0.93 1.00 1.00 0.25 0.67 0.80 1.00 1.00 0.50 1.00 0.82
Path
0.50 1.00 1.00 0.89 0.73 0.73 0.57 0.57 1.00 0.73 0.77 1.00 1.00 0.80 0.89 0.89 1.00 1.00 0.67 0.89 0.73 0.89 1.00 1.00 1.00 0.86 0.50 0.00 1.00 0.57 1.00 1.00 0.79
URL
0.67 0.80 1.00 N.A. 0.80 0.80 N.A. N.A. N.A. N.A. 0.81 1.00 0.67 0.73 N.A. 1.00 0.89 N.A. N.A. N.A. N.A. 0.86 1.00 1.00 0.75 N.A. 1.00 0.60 N.A. N.A. N.A. N.A. 0.87
IP Address
0.70 N.A. N.A. 0.73 0.94 0.89 N.A. 0.84 0.94 0.76 0.83 0.73 N.A. N.A. 0.84 0.94 0.84 N.A. 0.80 0.94 0.84 0.85 1.00 N.A. N.A. 0.82 0.75 0.75 N.A. 1.00 1.00 1.00 0.90
Port
0.74 N.A. N.A. 0.78 0.94 0.82 N.A. 0.94 N.A. 0.84 0.84 0.70 N.A. N.A. 0.82 0.82 0.67 N.A. 0.84 N.A. 0.94 0.80 0.94 N.A. N.A. 0.71 1.00 0.62 N.A. 0.75 N.A. 1.00 0.84
Permission
0.89 N.A. N.A. 0.80 0.78 1.00 N.A. N.A. N.A. N.A. 0.87 0.89 N.A. N.A. 0.80 0.82 1.00 N.A. N.A. N.A. N.A. 0.88 0.86 N.A. N.A. 0.50 0.50 0.40 N.A. N.A. N.A. N.A. 0.56
Range
Basic Numeric
0.73 1.00 0.75 0.73 0.60 0.67 1.00 0.89 1.00 0.80 0.82 0.67 0.80 0.89 0.46 0.50 0.67 1.00 0.67 0.89 0.80 0.73 0.75 1.00 0.89 0.36 0.00 0.50 0.67 0.75 0.50 1.00 0.64
Bool
1.00 0.75 1.00 0.80 1.00 1.00 N.A. 0.89 1.00 0.80 0.92 1.00 0.55 0.73 0.67 0.89 0.89 N.A. 0.73 1.00 0.80 0.80 1.00 0.86 1.00 0.00 0.67 0.00 N.A. 0.67 0.50 0.75 0.60
Enum
0.36 N.A. 0.86 0.73 0.67 0.89 0.89 0.75 0.80 N.A. 0.74 0.36 N.A. 0.89 1.00 0.86 0.75 0.89 0.89 0.80 N.A. 0.80 0.86 N.A. 1.00 0.75 0.57 0.60 0.75 1.00 0.75 N.A. 0.78
IP Address
0.70 N.A. N.A. 0.73 0.94 0.89 N.A. 0.84 0.94 0.76 0.83 0.73 N.A. N.A. 0.84 0.94 0.84 N.A. 0.80 0.94 0.84 0.85 1.00 N.A. N.A. 0.82 0.75 0.75 N.A. 1.00 1.00 1.00 0.90
Port
0.74 N.A. N.A. 0.78 0.94 0.82 N.A. 0.94 N.A. 0.84 0.84 0.70 N.A. N.A. 0.82 0.82 0.67 N.A. 0.84 N.A. 0.94 0.80 0.94 N.A. N.A. 0.71 1.00 0.62 N.A. 0.75 N.A. 1.00 0.84
Permission
0.89 N.A. N.A. 0.80 0.78 1.00 N.A. N.A. N.A. N.A. 0.87 0.89 N.A. N.A. 0.80 0.82 1.00 N.A. N.A. N.A. N.A. 0.88 0.86 N.A. N.A. 0.50 0.50 0.40 N.A. N.A. N.A. N.A. 0.56
Dependency Control
0.50 N.A. 0.00 0.00 0.00 0.25 0.00 0.00 0.00 N.A. 0.09 0.00 N.A. 0.29 0.00 0.00 0.00 0.00 0.00 0.00 N.A. 0.04 0.40 N.A. 0.67 0.29 0.00 0.33 0.00 0.40 0.00 N.A. 0.26
Value Relationship 1.00 N.A. N.A. 0.75 0.36 0.22 0.40 N.A. 0.50 N.A. 0.54 0.80 N.A. N.A. 0.67 0.29 0.25 0.40 N.A. 0.33 N.A. 0.46 0.00 N.A. N.A. 0.29 0.44 0.57 0.67 N.A. 0.67 N.A. 0.44
Version
Parameter Change 0.00 N.A. 0.29 0.44 0.36 0.00 0.29 0.33 0.00 N.A. 0.21 0.00 N.A. 0.75 0.00 0.75 0.00 0.00 0.80 0.57 N.A. 0.36 0.00 N.A. 0.67 0.00 0.29 0.00 0.00 0.00 0.00 N.A. 0.12
alluxio.underfs.gcs.default.mode=888
The property 'alluxio.underfs.gcs.default.mode' has 
the value '888' which is not a valid octal number.
Config.
A1:
The property 'alluxio.underfs.gcs.default.mode' has 
the value '888' which exceeds the range of an Integer.
A2:
Fig. 7: Correct and incorrect reasons returned by LLMs.
the reasons given by the LLMs are clear and explain the
misconfigurations. 3.1% of the answers contain a mix of correct
and incorrect reasons across queries. Ciri filters out incorrect
reasons using the voting mechanism (§III-B2) as correct reasons
are dominating. Figure 7 presents an example of mixed reasons,
with the second reason being an instance of hallucination.
In short, with frameworks like Ciri, LLMs can effectively
validate configurations for syntax or range violations; but are
less effective for the configurations that involve dependen-
cies between parameters and software versions, showing the
challenges for LLMs to reason about interactions between
parameters and between configuration and code [47]. To address
those misconfigurations, one can re-train or fine-tune LLMs
with data related to dependency and versions.
D. Biases
Finding 9. LLMs are biased to popular parameters: Ciri
is more effective in detecting misconfigurations of popular
parameters, but also reports more false alarms on them.
To measure the popularity of a configuration parameter, we
count the number of exact-match search results returned by
Google when searching the parameter name and call it G-hits.
We study the correlation between a parameter’s G-hits and
the effectiveness of LLMs in detecting the misconfigurations.
For each configuration file in the Misconfig dataset, we track the
frequency of LLMs detecting the parameter’s misconfigurations
with the ith highest G-hits in each file, where i = 1...8. We
separate cases when the misconfigured parameter is detected
versus missed. As shown in Figure 8, the median G-hits of
misconfigured parameters being detected is higher than the
median G-hits of misconfigured parameters being missed.
We also study the frequency of false alarms across different
ranking positions of G-hits within the file. Specifically, for each
configuration file in ValidConfig dataset across all ten projects
G-hits (log100)
Fig. 8: The G-hits distribution of the correctly detected
misconfigurations (orange), and the G-hits distribution of the
missed misconfigurations (blue). The bars in box plots indicate
medians. CL refers to CodeLlama.
1st
2nd
3rd
4th
5th
6th
7th
8th
1st
2nd
3rd
4th
5th
6th
7th
8th
CodeLlama
Claude-3
DeepSeek
GPT
# Times the param is identified during evaluation
Param of ith highest G-hits
Fig. 9: Frequency of the identified parameter with ith highest
G-hits in a configuration file.
we evaluated, we track the frequency of LLMs mistakenly
identifying the parameter with the ith highest G-hits in each
file, where i = 1...8. We group the results by the model family
as shown in Figure 9. The distributions reveal a clear skewness
towards parameters with higher G-hits, indicating that LLMs
are more prone to report false alarms on popular parameters.
The biases can be attributed to the training data of LLMs,
which are from public domains easily accessible by search
engines like Google. Topics or parameters that are popularly
discussed are more likely to be memorized by the LLMs,
due to more frequent presence in the training data. So,
for configuration validation, LLMs can be less effective for
parameters that are not commonly referenced online.
9

VI. THREATS TO VALIDITY
External threats. We evaluate Ciri with eight state-of-the-art
LLMs to mitigate threats on evaluated models. To mitigate
threats of evaluated projects, we select ten mature, widely used
projects of different types. These systems are commonly used in
prior studies [18], [20], [59], [60], [69], [73], [94]. To account
for bias in the evaluated configuration data, we include many
types of configuration parameters and their generation rules
based on prior work [32], [38], [39], [88]. Our results cannot
generalize to environment-related misconfigurations (discussed
in §VII). We expect the overall trend to be general, but the
precise numbers may vary with other LLMs, projects, and
configuration data in the field.
Internal threats. The internal threats lie in potential bugs
in the implementation of Ciri, and experimental scripts for
evaluation. We have rigorously reviewed our code and multiple
authors cross-validated the experiment results.
Construct threats. The threats to construct validity mainly lie
in metrics (§IV-B). We use the popular F1-score, precision, and
recall, and define our confusion matrices at both configuration
file and parameter levels.
VII. DISCUSSION AND FUTURE WORK
Improving effectiveness of LLMs as validators. Despite
the promising results, using LLMs directly as configuration
validators like Ciri is a starting point to harness the ability
of LLMs for configuration validation. Specifically, there are
circumstances where LLMs show limitations and biases (§V-C,
§V-D). One intricate aspect of configuration validation is
understanding configuration dependencies. Integrating LLMs
with configuration dependency analysis [18] could be beneficial.
We plan to investigate advanced prompting techniques, such
as Chain-of-Thoughts (CoT) [76], [79], [96]. For configuration
validation, CoT prompting can potentially mimic the reasoning
process of a human expert. By eliciting LLMs to generate
intermediate reasoning steps toward the validation results, it
makes the validation more transparent and potentially more
accurate. We also plan to explore extending Ciri into a multi-
agent framework, where Ciri can interact with additional tools
such as Ctest [69] and Cdep [18] through agent frameworks
such as LangChain [4] and AutoGen [80].
Lastly, integrating user feedback loops can be valuable. With
user feedback on validation results, the iterative procedure can
refine LLMs over time, leading to more accurate responses.
Detecting environment-related misconfigurations. While
our study primarily targets misconfigurations that are common
in the field [90], the validity of configuration files can vary
across environments. For instance, a configuration parameter
can specify a file path, so the file’s existence, permission, and
content decide its validity. To address these, LLMs can generate
environment-specific scripts to run in the target environment.
For example, given the configuration file as input, the LLM
can generate a Python script as follows.
Such LLM-generated scripts can help identify issues like
misconfigured paths, unreachable addresses, missing packages,
try:
with open("/path/to/file", "r") as f:
data = json.loads(f.read())
print("Valid configuration")
except:
print("Invalid configuration")
or invalid permissions. Notably, these scripts offer a lightweight
alternative to configuration tests [20], [87].
Detecting source-code related misconfigurations. We ex-
plored augmenting LLMs with code snippets (Finding 5), which
can reveal parameter types and semantics. This approach can
be further improved by integrating advanced program analysis
to present both configuration and relevant source code to the
LLM. Techniques like static or dynamic program slicing [9],
[59], [86], [91] can help identify the relevant code.
Fine-tuning LLMs for configuration validation. We also plan
to explore fine-tuning to tackle system-specific configuration
problems, which is hard to address with common-sense knowl-
edge. Specifically, configuration related software evolution is
prevalent, which introduces new parameters and changes the
semantics and constraints of existing parameters [93], [94]. A
promising solution is to fine-tune LLMs on new code/data, and
make LLMs evolution-aware.
VIII. RELATED WORK
Configuration validation. Prior studies developed frameworks
for developers to implement validators [12], [29], [58], [70]
and test cases [69], [87], as well as techniques to extract
configuration constraints [41], [50], [88], [91]. However, manu-
ally writing validators and tests requires extensive engineering
efforts, and is hard to cover various properties of different
configurations [32], [38], [39], [86], [88]. ML/NLP-based
configuration validation techniques have been investigated.
Traditional ML/NLP-based approaches learn correctness rules
from configuration data [14], [35], [56], [65], [66], [72], [77],
[92] and documents [57], [83] and then use the learned rules
for validation. These techniques face data challenges and rely
on predefined learning features and models, making them hard
to generalize to different projects and deployment scenarios.
We explore using LLMs for configuration validation, which
can potentially address the limitations of traditional ML/NLP
techniques towards automatic, effective validation solutions.
LLMs for software engineering. LLMs are actively applied
to software engineering tasks, where they have demonstrated ef-
fectiveness in generating, summarizing, and translating code [6],
[17], [31], [40], [43], [62], [63], failure diagnosis [7], [19], fault
localization and program repair [23], [49], [81], [82]. LLMs
for code are also increasingly prominent [17], [24], [25], [52],
[53], [84], and are used for coding tasks. We take a first step
to apply LLMs for software configuration problems, and show
that LLMs have the potential to efficiently automate certain
validation tasks and even bring advances over developer-written
validators. Ciri as a framework is generic to different LLMs.
10

IX. CONCLUDING REMARKS
As a first step to harvest LLMs for software configuration,
we develop Ciri as an open platform to experiment with LLMs
as configuration validators, and present the important design
choices. Through Ciri, we analyze LLM-empowered configura-
tion validators. Our analysis shows the promising effectiveness
of state-of-the-art LLMs as configuration validators, as well as
their limitations. Our work shed light on new, exciting research
directions of using LLMs for software configuration research.
REFERENCES
[1] Openctest. https://github.com/xlab-uiuc/openctest, 2020.
[2] ChatGPT. https://openai.com/blog/chatgpt, 2022.
[3] Codex. https://openai.com/blog/openai-codex, 2022.
[4] Langchain. https://github.com/langchain-ai/langchain, 2022.
[5] ADIWARDANA, D., LUONG, M.-T., SO, D. R., HALL, J., FIEDEL, N.,
THOPPILAN, R., YANG, Z., KULSHRESHTHA, A., NEMADE, G., LU, Y.,
ET AL. Towards a human-like open-domain chatbot. arXiv:2001.09977
(2020).
[6] AHMED, T., AND DEVANBU, P. Few-shot training LLMs for project-
specific code-summarization. In ASE (2022).
[7] AHMED, T., GHOSH, S., BANSAL, C., ZIMMERMANN, T., ZHANG, X.,
AND RAJMOHAN, S. Recommending Root-Cause and Mitigation Steps
for Cloud Incidents Using Large Language Models. In ICSE (2023).
[8] ANTHROPIC. Introducing 100k context windows. https://www.anthropic.
com/index/100k-context-windows, 2023.
[9] ATTARIYAN, M., AND FLINN, J. Automating configuration troubleshoot-
ing with dynamic information flow analysis. In OSDI (2010).
[10] BANG, Y., CAHYAWIJAYA, S., LEE, N., DAI, W., SU, D., WILIE,
B., LOVENIA, H., JI, Z., YU, T., CHUNG, W., ET AL. A multitask,
multilingual, multimodal evaluation of chatgpt on reasoning, hallucination,
and interactivity. arXiv:2302.04023 (2023).
[11] BARROSO, L. A., H ¨OLZLE, U., AND RANGANATHAN, P. The Datacenter
as a Computer: Designing Warehouse-Scale Machines, 3 ed. Morgan
and Claypool Publishers, 2018.
[12] BASET, S., SUNEJA, S., BILA, N., TUNCER, O., AND ISCI, C. Usable
Declarative Configuration Specification and Validation for Applications,
Systems, and Cloud. In Middleware (2017).
[13] BEYER, B., MURPHY, N. R., RENSIN, D. K., KAWAHARA, K., AND
THORNE, S. Site Reliability Workbook: Practical Ways to Implement
SRE. O’Reilly Media Inc., 2018.
[14] BHAGWAN, R., MEHTA, S., RADHAKRISHNA, A., AND GARG, S.
Learning Patterns in Configuration. In ASE (2021).
[15] BROWN, T., MANN, B., RYDER, N., SUBBIAH, M., KAPLAN, J. D.,
DHARIWAL, P., NEELAKANTAN, A., SHYAM, P., SASTRY, G., ASKELL,
A., ET AL. Language models are few-shot learners. arXiv:2005.14165
(2020).
[16] CAMBURU, O.-M., SHILLINGFORD, B., MINERVINI, P., LUKASIEWICZ,
T., AND BLUNSOM, P. Make up your mind! adversarial generation of
inconsistent natural language explanations. arXiv:1910.03065 (2019).
[17] CHEN, M., TWOREK, J., JUN, H., YUAN, Q., PINTO, H. P. D. O.,
KAPLAN, J., EDWARDS, H., BURDA, Y., JOSEPH, N., BROCKMAN,
G., ET AL.
Evaluating large language models trained on code.
arXiv:2107.03374 (2021).
[18] CHEN, Q., WANG, T., LEGUNSEN, O., LI, S., AND XU, T. Understand-
ing and Discovering Software Configuration Dependencies in Cloud and
Datacenter Systems. In ESEC/FSE (2020).
[19] CHEN, Y., XIE, H., MA, M., KANG, Y., GAO, X., SHI, L., CAO,
Y., GAO, X., FAN, H., WEN, M., ET AL.
Empowering Practical
Root Cause Analysis by Large Language Models for Cloud Incidents.
arXiv:2305.15778 (2023).
[20] CHENG, R., ZHANG, L., MARINOV, D., AND XU, T.
Test-Case
Prioritization for Configuration Testing. In ISSTA (2021).
[21]
CONTRIBUTORS, W. tf–idf. https://en.wikipedia.org/wiki/Tf%E2%80%
93idf, 2024.
[22] ELAZAR, Y., KASSNER, N., RAVFOGEL, S., RAVICHANDER, A., HOVY,
E., SCH ¨UTZE, H., AND GOLDBERG, Y.
Measuring and improving
consistency in pretrained language models. arXiv:2102.01017 (2021).
[23] FAN, Z., GAO, X., MIRCHEV, M., ROYCHOUDHURY, A., AND TAN,
S. H. Automated repair of programs from large language models. In
ICSE (2023).
[24] FENG, Z., GUO, D., TANG, D., DUAN, N., FENG, X., GONG, M., SHOU,
L., QIN, B., LIU, T., JIANG, D., ET AL. Codebert: A pre-trained model
for programming and natural languages. arXiv:2002.08155 (2020).
[25] FRIED, D., AGHAJANYAN, A., LIN, J., WANG, S., WALLACE, E.,
SHI, F., ZHONG, R., TAU YIH, W., ZETTLEMOYER, L., AND LEWIS,
M.
InCoder: A Generative Model for Code Infilling and Synthesis.
arXiv:2204.05999 (2023).
[26] GUNAWI, H. S., HAO, M., SUMINTO, R. O., LAKSONO, A., SATRIA,
A. D., ADITYATAMA, J., AND ELIAZAR, K. J. Why Does the Cloud
Stop Computing? Lessons from Hundreds of Service Outages. In SOCC
(2016).
[27] GUU, K., LEE, K., TUNG, Z., PASUPAT, P., AND CHANG, M.-W. Realm:
Retrieval-augmented language model pre-training. arXiv:2002.08909
(2020).
[28] HUANG, J., AND CHANG, K. C.-C.
Towards Reasoning in Large
Language Models: A Survey. In Findings of ACL (2023).
[29] HUANG, P., BOLOSKY, W. J., SIGH, A., AND ZHOU, Y. ConfValley: A
Systematic Configuration Validation Framework for Cloud Services. In
EuroSys (2015).
[30] HUANG, Q., WANG, H. J., AND BORISOV, N. Privacy-Preserving Friends
Troubleshooting Network. In NDSS (2005).
[31] IYER, S., KONSTAS, I., CHEUNG, A., AND ZETTLEMOYER, L. Mapping
language to code in programmatic context. arXiv:1808.09588 (2018).
[32] KELLER, L., UPADHYAYA, P., AND CANDEA, G. ConfErr: A Tool for
Assessing Resilience to Human Configuration Errors. In DSN (2008).
[33] KENDRICK, S. What Takes Us Down? USENIX ;login: (2012).
[34] KICIMAN, E., AND WANG, Y.-M. Discovering Correctness Constraints
for Self-Management of System Configuration. In ICAC (2004).
[35] LE, F., LEE, S., WONG, T., KIM, H. S., AND NEWCOMB, D. Minerals:
Using Data Mining to Detect Router Misconfigurations. Tech. Rep.
CMU-CyLab-06-008, Carnegie Mellon University, 2006.
[36] LEUSCHNER, L., K ¨UTTLER, M., STUMPF, T., BAIER, C., H ¨ARTIG, H.,
AND KL ¨UPPELHOLZ, S. Towards Automated Configuration of Systems
with Non-Functional Constraints. In HotOS-XVI (2017).
[37] LEWIS, P., PEREZ, E., PIKTUS, A., PETRONI, F., KARPUKHIN, V.,
GOYAL, N., K ¨UTTLER, H., LEWIS, M., TAU YIH, W., ROCKT ¨ASCHEL,
T., RIEDEL, S., AND KIELA, D. Retrieval-augmented generation for
knowledge-intensive nlp tasks. arXiv:2005.11401 (2021).
[38] LI, S., LI, W., LIAO, X., PENG, S., ZHOU, S., JIA, Z., AND WANG,
T.
ConfVD: System Reactions Analysis and Evaluation Through
Misconfiguration Injection. IEEE Transactions on Reliability (2018).
[39] LI, W., JIA, Z., LI, S., ZHANG, Y., WANG, T., XU, E., WANG, J., AND
LIAO, X. Challenges and Opportunities: An In-Depth Empirical Study
on Configuration Error Injection Testing. In ISSTA (2021).
[40] LI, Y., CHOI, D., CHUNG, J., KUSHMAN, N., SCHRITTWIESER, J.,
LEBLOND, R., ECCLES, T., KEELING, J., GIMENO, F., DAL LAGO,
A., ET AL. Competition-level code generation with alphacode. Science
(2022).
[41] LIAO, X., ZHOU, S., LI, S., JIA, Z., LIU, X., AND HE, H. Do You Really
Know How to Configure Your Software? Configuration Constraints in
Source Code May Help. IEEE Transactions on Reliability (2018).
[42] LIU, Y., YAO, Y., TON, J.-F., ZHANG, X., GUO, R., CHENG, H.,
KLOCHKOV, Y., TAUFIQ, M. F., AND LI, H.
Trustworthy llms: a
survey and guideline for evaluating large language models’ alignment.
arXiv:2308.05374 (2023).
[43] LU, S., GUO, D., REN, S., HUANG, J., SVYATKOVSKIY, A., BLANCO,
A., CLEMENT, C., DRAIN, D., JIANG, D., TANG, D., ET AL. Codexglue:
A machine learning benchmark dataset for code understanding and
generation. arXiv:2102.04664 (2021).
[44] MANAKUL, P., LIUSIE, A., AND GALES, M. J. F. Selfcheckgpt: Zero-
resource black-box hallucination detection for generative large language
models. arXiv:2303.08896 (2023).
[45] MAURER, B. Fail at Scale: Reliability in the Face of Rapid Change.
Communications of the ACM (2015).
[46] MEHTA, S., BHAGWAN, R., KUMAR, R., ASHOK, B., BANSAL, C.,
MADDILA, C., BIRD, C., ASTHANA, S., AND KUMAR, A.
Rex:
Preventing Bugs and Misconfiguration in Large Services using Correlated
Change Analysis. In NSDI (2020).
[47] MEINICKE, J., WONG, C.-P., K ¨ASTNER, C., TH ¨UM, T., AND SAAKE,
G. On Essential Configuration Complexity: Measuring Interations in
Highly-Configurable Systems. In ASE (2016).
11

[48] MIN, S., LYU, X., HOLTZMAN, A., ARTETXE, M., LEWIS, M.,
HAJISHIRZI, H., AND ZETTLEMOYER, L.
Rethinking the role of
demonstrations: What makes in-context learning work? arXiv:2202.12837
(2022).
[49] MIRSKY, Y., MACON, G., BROWN, M., YAGEMANN, C., PRUETT, M.,
DOWNING, E., MERTOGUNO, S., AND LEE, W. Vulchecker: Graph-
based vulnerability localization in source code. In USENIX Security
(2023).
[50] NADI, S., BERGER, T., K ¨ASTNER, C., AND CZARNECKI, K. Where
do configuration constraints stem from? an extraction approach and an
empirical study. TSE (2015).
[51] NAKANO, R., HILTON, J., BALAJI, S., WU, J., OUYANG, L., KIM,
C., HESSE, C., JAIN, S., KOSARAJU, V., SAUNDERS, W., JIANG, X.,
COBBE, K., ELOUNDOU, T., KRUEGER, G., BUTTON, K., KNIGHT, M.,
CHESS, B., AND SCHULMAN, J. Webgpt: Browser-assisted question-
answering with human feedback. arXiv:2112.09332 (2022).
[52] NEDELKOSKI, S., CARDOSO, J., AND KAO, O. Anomaly detection from
system tracing data using multimodal deep learning. In CLOUD (2019).
[53] NIJKAMP, E., PANG, B., HAYASHI, H., TU, L., WANG, H., ZHOU, Y.,
SAVARESE, S., AND XIONG, C. Codegen: An open large language model
for code with multi-turn program synthesis. arXiv:2203.13474 (2022).
[54] OPENAI. Gpt-4 technical report. arXiv:2303.08774 (2023).
[55] OPPENHEIMER, D., GANAPATHI, A., AND PATTERSON, D. A. Why
Do Internet Services Fail, and What Can Be Done About It? In USITS
(2003).
[56] PALATIN, N., LEIZAROWITZ, A., SCHUSTER, A., AND WOLFF, R.
Mining for Misconfigured Machines in Grid Systems. In KDD (2006).
[57] POTHARAJU, R., CHAN, J., HU, L., NITA-ROTARU, C., WANG, M.,
ZHANG, L., AND JAIN, N. ConfSeer: Leveraging Customer Support
Knowledge Bases for Automated Misconfiguration Detection. In VLDB
(2015).
[58] RAAB, M., AND BARANY, G.
Challenges in Validating FLOSS
Configuration. In OSS (2017).
[59] RABKIN, A., AND KATZ, R. Precomputing Possible Configuration Error
Diagnosis. In ASE (2011).
[60] RABKIN, A., AND KATZ, R. Static Extraction of Program Configuration
Options. In ICSE (2011).
[61] RABKIN, A., AND KATZ, R. How Hadoop Clusters Break. IEEE Software
Magazine (2013).
[62] ROZIERE, B., LACHAUX, M.-A., CHANUSSOT, L., AND LAMPLE, G.
Unsupervised translation of programming languages. Advances in Neural
Information Processing Systems (2020).
[63] ROZIERE, B., ZHANG, J. M., CHARTON, F., HARMAN, M., SYNNAEVE,
G., AND LAMPLE, G. Leveraging automated unit tests for unsupervised
code translation. arXiv:2110.06773 (2021).
[64] ROZI`ERE, B., GEHRING, J., GLOECKLE, F., SOOTLA, S., GAT, I.,
TAN, X. E., ADI, Y., LIU, J., SAUVESTRE, R., REMEZ, T., RAPIN,
J., KOZHEVNIKOV, A., EVTIMOV, I., BITTON, J., BHATT, M., FERRER,
C. C., GRATTAFIORI, A., XIONG, W., D´EFOSSEZ, A., COPET, J.,
AZHAR, F., TOUVRON, H., MARTIN, L., USUNIER, N., SCIALOM, T.,
AND SYNNAEVE, G. Code llama: Open foundation models for code,
2024.
[65] SANTOLUCITO, M., ZHAI, E., DHODAPKAR, R., SHIM, A., AND
PISKAC, R. Synthesizing Configuration File Specifications with As-
sociation Rule Learning. In OOPSLA (2017).
[66] SANTOLUCITO, M., ZHAI, E., AND PISKAC, R. Probabilistic Automated
Language Learning for Configuration Files. In CAV (2016).
[67] SAYAGH, M., KERZAZI, N., ADAMS, B., AND PETRILLO, F.
Soft-
ware Configuration Engineering in Practice: Interviews, Surveys, and
Systematic Literature Review. TSE (2018).
[68] SHERMAN, A., LISIECKI, P., BERKHEIMER, A., AND WEIN, J. ACMS:
Akamai Configuration Management System. In NSDI (2005).
[69] SUN, X., CHENG, R., CHEN, J., ANG, E., LEGUNSEN, O., AND XU, T.
Testing Configuration Changes in Context to Prevent Production Failures.
In OSDI (2020).
[70] TANG, C., KOOBURAT, T., VENKATACHALAM, P., CHANDER, A.,
WEN, Z., NARAYANAN, A., DOWELL, P., AND KARL, R.
Holistic
Configuration Management at Facebook. In SOSP (2015).
[71] THAKUR, S., AHMAD, B., PEARCE, H., TAN, B., DOLAN-GAVITT, B.,
KARRI, R., AND GARG, S. VeriGen: A Large Language Model for
Verilog Code Generation. arXiv:2308.00708 (2023).
[72] WANG, H. J., PLATT, J. C., CHEN, Y., ZHANG, R., AND WANG, Y.-M.
Automatic Misconfiguration Troubleshooting with PeerPressure. In OSDI
(2004).
[73] WANG, S., LIAN, X., MARINOV, D., AND XU, T. Test Selection for
Unified Regression Testing. In ICSE (2023).
[74] WANG, T., HE, H., LIU, X., LI, S., JIA, Z., JIANG, Y., LIAO, Q., AND
LI, W. ConfTainter: Static Taint Analysis For Configuration Options. In
ASE (2023).
[75] WANG, T., JIA, Z., LI, S., ZHENG, S., YU, Y., XU, E., PENG, S., AND
LIAO, X. Understanding and Detecting On-the-Fly Configuration Bugs.
In ICSE (2023).
[76] WANG, X., WEI, J., SCHUURMANS, D., LE, Q., CHI, E., NARANG, S.,
CHOWDHERY, A., AND ZHOU, D. Self-Consistency Improves Chain of
Thought Reasoning in Language Models. arXiv:2203.11171 (2023).
[77] WANG, Y.-M., VERBOWSKI, C., DUNAGAN, J., CHEN, Y., WANG,
H. J., YUAN, C., AND ZHANG, Z. STRIDER: A Black-box, State-based
Approach to Change and Configuration Management and Support. In
LISA (2003).
[78] WEI, J., WANG, X., SCHUURMANS, D., BOSMA, M., ICHTER, B., XIA,
F., CHI, E., LE, Q., AND ZHOU, D. Chain-of-Thought Prompting Elicits
Reasoning in Large Language Models. arXiv:2201.11903 (2023).
[79] WEI, J., WANG, X., SCHUURMANS, D., BOSMA, M., XIA, F., CHI,
E., LE, Q. V., ZHOU, D., ET AL. Chain-of-thought prompting elicits
reasoning in large language models. Advances in Neural Information
Processing Systems (2022).
[80] WU, Q., BANSAL, G., ZHANG, J., WU, Y., LI, B., ZHU, E., JIANG,
L., ZHANG, X., ZHANG, S., LIU, J., AWADALLAH, A. H., WHITE,
R. W., BURGER, D., AND WANG, C. Autogen: Enabling next-gen llm
applications via multi-agent conversation, 2023.
[81] XIA, C. S., PALTENGHI, M., TIAN, J. L., PRADEL, M., AND ZHANG, L.
Universal fuzzing via large language models. arXiv:2308.04738 (2023).
[82] XIA, C. S., WEI, Y., AND ZHANG, L. Automated Program Repair in
the Era of Large Pre-Trained Language Models. In ICSE (2023).
[83] XIANG, C., HUANG, H., YOO, A., ZHOU, Y., AND PASUPATHY, S.
PracExtractor: Extracting Configuration Good Practices from Manuals
to Detect Server Misconfigurations. In ATC (2020).
[84] XU, F. F., ALON, U., NEUBIG, G., AND J.HELLENDOORN, V. A System-
atic Evaluation of Large Language Models of Code. arXiv:2202.13169
(2022).
[85] XU, T., JIN, L., FAN, X., ZHOU, Y., PASUPATHY, S., AND TALWADKER,
R. Hey, You Have Given Me Too Many Knobs! Understanding and
Dealing with Over-Designed Configuration in System Software.
In
ESEC/FSE (2015).
[86] XU, T., JIN, X., HUANG, P., ZHOU, Y., LU, S., JIN, L., AND
PASUPATHY, S.
Early Detection of Configuration Errors to Reduce
Failure Damage. In OSDI (2016).
[87] XU, T., AND LEGUNSEN, O. Configuration Testing: Testing Configura-
tion Values as Code and with Code. arXiv:1905.12195 (2019).
[88] XU, T., ZHANG, J., HUANG, P., ZHENG, J., SHENG, T., YUAN, D.,
ZHOU, Y., AND PASUPATHY, S. Do Not Blame Users for Misconfigura-
tions. In SOSP (2013).
[89] XU, T., AND ZHOU, Y. Systems Approaches to Tackling Configuration
Errors: A Survey. ACM Computing Surveys 47, 4 (2015).
[90] YIN, Z., MA, X., ZHENG, J., ZHOU, Y., BAIRAVASUNDARAM, L. N.,
AND PASUPATHY, S. An Empirical Study on Configuration Errors in
Commercial and Open Source Systems. In SOSP (2011).
[91] ZHANG, J., PISKAC, R., ZHAI, E., AND XU, T. Static Detection of
Silent Misconfigurations with Deep Interaction Analysis. In OOPSLA
(2021).
[92] ZHANG, J., RENGANARAYANA, L., ZHANG, X., GE, N., BALA, V.,
XU, T., AND ZHOU, Y. EnCore: Exploiting System Environment and
Correlation Information for Misconfiguration Detection. In ASPLOS
(2014).
[93] ZHANG, S., AND ERNST, M. D. Which Configuration Option Should I
Change? In ICSE (2014).
[94] ZHANG, Y., HE, H., LEGUNSEN, O., LI, S., DONG, W., AND XU, T.
An Evolutionary Study of Configuration Design and Implementation in
Cloud Systems. In ICSE (2021).
[95] ZHANG, Y., LI, Y., CUI, L., CAI, D., LIU, L., FU, T., HUANG, X.,
ZHAO, E., ZHANG, Y., CHEN, Y., ET AL. Siren’s Song in the AI Ocean:
A Survey on Hallucination in Large Language Models. arXiv:2309.01219
(2023).
[96] ZHANG, Z., ZHANG, A., LI, M., AND SMOLA, A. Automatic chain of
thought prompting in large language models. arXiv:2210.03493 (2022).
[97] ZIEGLER, D. M., STIENNON, N., WU, J., BROWN, T. B., RADFORD, A.,
AMODEI, D., CHRISTIANO, P., AND IRVING, G. Fine-tuning language
models from human preferences. arXiv:1909.08593 (2019).
12

